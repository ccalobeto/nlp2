{
  
    
        "post0": {
            "title": "NLP in Action",
            "content": "System tools . Install a pip package in the current Jupyter kernel . # !{sys.executable} -m pip install vaderSentiment . NLP Tools . Regular Expressions . import re . basics of regular expressions . [] indicates character class | r&#39;[ s]&#39; is equivalent to r&#39; t n r x0b x0c&#39; match all the spaces, tabs, returns, new lines and form-feed | r&#39;[a-z]&#39; match all lowercase | r&#39;[0-9]&#39; match any digit | r&#39;[_a-zA-Z]&#39; match any underscore character or letter of the english alphabet | . Match any sentence that begins with hi|hello|hey followed by space(s) and a word . r = &quot;(hi|hello|hey)[ ]*([a-z]*)&quot; . print(re.match(r, &#39;Hello Rosa&#39;, flags=re.IGNORECASE)) print(re.match(r, &quot;hi ho, hi ho, it&#39;s off to work ...&quot;, flags=re.IGNORECASE)) print(re.match(r, &quot;hey, what&#39;s up&quot;, flags=re.IGNORECASE)) . &lt;re.Match object; span=(0, 10), match=&#39;Hello Rosa&#39;&gt; None &lt;re.Match object; span=(0, 9), match=&#39;hey, what&#39;&gt; . Example with a complex pattern . r = r&quot;[^a-z]*([y]o|[h&#39;]?ello|ok|hey|(good[ ])?(morn[gin&#39;]{0,3}|&quot; r&quot;afternoon|even[gin&#39;]{0,3}))[ s,;:]{1,3}([a-z]{1,20})&quot; re_greeting = re.compile(r, flags=re.IGNORECASE) . print(re_greeting.match(&#39;Hello Rosa&#39;)) print(re_greeting.match(&#39;Hello Rosa&#39;).groups()) print(re_greeting.match(&quot;Good morning Rosa&quot;)) print(re_greeting.match(&quot;Good Manning Rosa&quot;)) print(re_greeting.match(&#39;Good evening Rosa Parks&#39;).groups()) print(re_greeting.match(&quot;Good Morn&#39;n Rosa&quot;)) print(re_greeting.match(&quot;yo Rosa&quot;)) . &lt;re.Match object; span=(0, 10), match=&#39;Hello Rosa&#39;&gt; (&#39;Hello&#39;, None, None, &#39;Rosa&#39;) &lt;re.Match object; span=(0, 17), match=&#39;Good morning Rosa&#39;&gt; None (&#39;Good evening&#39;, &#39;Good &#39;, &#39;evening&#39;, &#39;Rosa&#39;) &lt;re.Match object; span=(0, 16), match=&#34;Good Morn&#39;n Rosa&#34;&gt; &lt;re.Match object; span=(0, 7), match=&#39;yo Rosa&#39;&gt; . Simple Chat Bot . Enter the text &#39;good morning rosa or hello rose&#39; . my_names = set([&#39;rosa&#39;, &#39;rose&#39;, &#39;chatty&#39;, &#39;chatbot&#39;, &#39;bot&#39;, &#39;chatterbot&#39;]) curt_names = set([&#39;hal&#39;, &#39;you&#39;, &#39;u&#39;]) greeter_name = &#39;&#39; match = re_greeting.match(input()) if match: at_name = match.groups()[-1] if at_name in curt_names: print(&quot;Good one.&quot;) elif at_name.lower() in my_names: print(&quot;Hi {}, How are you?&quot;.format(greeter_name)) . Hi , How are you? . Word Permutations . n=3 permutations with text &#39;Good morning Rosa!&#39; . from itertools import permutations print([&quot; &quot;.join(combo) for combo in permutations(&quot;Good morning Rosa!&quot;.split(), 3)]) . [&#39;Good morning Rosa!&#39;, &#39;Good Rosa! morning&#39;, &#39;morning Good Rosa!&#39;, &#39;morning Rosa! Good&#39;, &#39;Rosa! Good morning&#39;, &#39;Rosa! morning Good&#39;] . Count Words . Counting words using Counter. Dict output . from collections import Counter print(Counter(&quot;Guten Morgen Rosa&quot;.split())) print(Counter(&quot;Good morning morning , Rosa!&quot;.split())) . Counter({&#39;Guten&#39;: 1, &#39;Morgen&#39;: 1, &#39;Rosa&#39;: 1}) Counter({&#39;morning&#39;: 2, &#39;Good&#39;: 1, &#39;,&#39;: 1, &#39;Rosa!&#39;: 1}) . Word Tokenization . Tokens . split into tokens . sentence = &quot;Thomas Jefferson began building Monticello at age of 26.&quot; sentence.split() str.split(sentence) . [&#39;Thomas&#39;, &#39;Jefferson&#39;, &#39;began&#39;, &#39;building&#39;, &#39;Monticello&#39;, &#39;at&#39;, &#39;age&#39;, &#39;of&#39;, &#39;26.&#39;] . split a sentence into tokens, order it and convert to a vector . import numpy as np token_sequence = str.split(sentence) vocab = sorted(set(token_sequence)) num_tokens = len(token_sequence) vocab_size = len(vocab) onehot_vectors = np.zeros((num_tokens, vocab_size), int) for i, word in enumerate(token_sequence): onehot_vectors[i, vocab.index(word)] = 1 print(&#39;, &#39;.join(vocab)) print(&#39;*********************&#39;) print(onehot_vectors) . 26., Jefferson, Monticello, Thomas, age, at, began, building, of ********************* [[0 0 0 1 0 0 0 0 0] [0 1 0 0 0 0 0 0 0] [0 0 0 0 0 0 1 0 0] [0 0 0 0 0 0 0 1 0] [0 0 1 0 0 0 0 0 0] [0 0 0 0 0 1 0 0 0] [0 0 0 0 1 0 0 0 0] [0 0 0 0 0 0 0 0 1] [1 0 0 0 0 0 0 0 0]] . Building a dataframe with matrix of vectors . import pandas as pd print(pd.DataFrame(onehot_vectors, columns=vocab)) . 26. Jefferson Monticello Thomas age at began building of 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 2 0 0 0 0 0 0 1 0 0 3 0 0 0 0 0 0 0 1 0 4 0 0 1 0 0 0 0 0 0 5 0 0 0 0 0 1 0 0 0 6 0 0 0 0 1 0 0 0 0 7 0 0 0 0 0 0 0 0 1 8 1 0 0 0 0 0 0 0 0 . construct a frequency vector . sentences = &quot;&quot;&quot;Thomas Jefferson began building Monticello at the age of 26. n&quot;&quot;&quot; sentences += &quot;&quot;&quot;Construction was done mostly by local masons and carpenters. n&quot;&quot;&quot; sentences += &quot;He moved into the South Pavilion in 1770. n&quot; sentences += &quot;&quot;&quot;Turning Monticello into a neoclassical masterpiece was Jefferson&#39;s obsession.&quot;&quot;&quot; print(&#39;text to analize:&#39;, sentences) print(&#39;*********************************************************&#39;) #1. construct a dict of dicts corpus = {} for i, sent in enumerate(sentences.split(&#39; n&#39;)): corpus[&#39;sent{}&#39;.format(i)] = dict((tok, 1) for tok in sent.split()) print(&#39;first dict:&#39;, corpus[&#39;sent0&#39;]) print(&#39;*********************************************************&#39;) #2. convert dict to dataframe df = pd.DataFrame.from_records(corpus).fillna(0).astype(int).T print(df[df.columns[:10]]) # Thomas Jefferson began building Monticello at the age of 26. # sent0 1 1 1 1 1 1 1 1 1 1 # sent1 0 0 0 0 0 0 0 0 0 0 # sent2 0 0 0 0 0 0 1 0 0 0 # sent3 0 0 0 0 1 0 0 0 0 0 print(&#39;**********************************************************&#39;) print(&#39;word shared by two sentences:&#39;,[(k, v) for (k, v) in (df.loc[&#39;sent0&#39;] &amp; df.loc[&#39;sent3&#39;]). items() if v]) . text to analize: Thomas Jefferson began building Monticello at the age of 26. Construction was done mostly by local masons and carpenters. He moved into the South Pavilion in 1770. Turning Monticello into a neoclassical masterpiece was Jefferson&#39;s obsession. ********************************************************* first dict: {&#39;Thomas&#39;: 1, &#39;Jefferson&#39;: 1, &#39;began&#39;: 1, &#39;building&#39;: 1, &#39;Monticello&#39;: 1, &#39;at&#39;: 1, &#39;the&#39;: 1, &#39;age&#39;: 1, &#39;of&#39;: 1, &#39;26.&#39;: 1} ********************************************************* Thomas Jefferson began building Monticello at the age of 26. sent0 1 1 1 1 1 1 1 1 1 1 sent1 0 0 0 0 0 0 0 0 0 0 sent2 0 0 0 0 0 0 1 0 0 0 sent3 0 0 0 0 1 0 0 0 0 0 ********************************************************** word shared by two sentences: [(&#39;Monticello&#39;, 1)] . split into tokens using regular expressions . import re sentence = &quot;&quot;&quot;Thomas Jefferson began building Monticello at the age of 26.&quot;&quot;&quot; tokens = re.split(r&#39;[- s.,;!?]+&#39;, sentence) print(&#39;list of tokens:&#39;, tokens) print(&#39;***********************************************&#39;) # a better version of tokenization fast and manageable pattern = re.compile(r&quot;([- s.,;!?])+&quot;) tokens = pattern.split(sentence) print(&#39;last 10 tokens:&#39;, tokens[-10:]) # [&#39;the&#39;, &#39; &#39;, &#39;age&#39;, &#39; &#39;, &#39;of&#39;, &#39; &#39;, &#39;26&#39;, &#39;.&#39;, &#39;&#39;] . list of tokens: [&#39;Thomas&#39;, &#39;Jefferson&#39;, &#39;began&#39;, &#39;building&#39;, &#39;Monticello&#39;, &#39;at&#39;, &#39;the&#39;, &#39;age&#39;, &#39;of&#39;, &#39;26&#39;, &#39;&#39;] *********************************************** last 10 tokens: [&#39; &#39;, &#39;the&#39;, &#39; &#39;, &#39;age&#39;, &#39; &#39;, &#39;of&#39;, &#39; &#39;, &#39;26&#39;, &#39;.&#39;, &#39;&#39;] . ignoring punctuation . print(&#39;removing punctuation:&#39;, [x for x in tokens if x not in &#39;- t n.,;!?&#39;]) . removing punctuation: [&#39;Thomas&#39;, &#39;Jefferson&#39;, &#39;began&#39;, &#39;building&#39;, &#39;Monticello&#39;, &#39;at&#39;, &#39;the&#39;, &#39;age&#39;, &#39;of&#39;, &#39;26&#39;] . ignoring whitespaces with tokeneizer . from nltk.tokenize import RegexpTokenizer tokenizer = RegexpTokenizer(r&#39; w+|$[0-9.]+| S+&#39;) print(tokenizer.tokenize(sentence)) . [&#39;Thomas&#39;, &#39;Jefferson&#39;, &#39;began&#39;, &#39;building&#39;, &#39;Monticello&#39;, &#39;at&#39;, &#39;the&#39;, &#39;age&#39;, &#39;of&#39;, &#39;26&#39;, &#39;.&#39;] . managing contractions with tokeneizer . from nltk.tokenize import TreebankWordTokenizer sentence = &quot;&quot;&quot;Monticello wasn&#39;t designated as UNESCO World Heritage Site until 1987.&quot;&quot;&quot; tokenizer = TreebankWordTokenizer() print(tokenizer.tokenize(sentence)) . [&#39;Monticello&#39;, &#39;was&#39;, &#34;n&#39;t&#34;, &#39;designated&#39;, &#39;as&#39;, &#39;UNESCO&#39;, &#39;World&#39;, &#39;Heritage&#39;, &#39;Site&#39;, &#39;until&#39;, &#39;1987&#39;, &#39;.&#39;] . tokenize informal conversation . from nltk.tokenize.casual import casual_tokenize message = &quot;&quot;&quot;RT @TJMonticello Best day everrrrrrr at Monticello. Awesommmmmmeeeeeeee day :*)&quot;&quot;&quot; print(&#39;tokens:&#39;, casual_tokenize(message)) print(&#39;***************************************************&#39;) print(&#39;best approach to tokens:&#39;, casual_tokenize(message, reduce_len=True, strip_handles=True)) . tokens: [&#39;RT&#39;, &#39;@TJMonticello&#39;, &#39;Best&#39;, &#39;day&#39;, &#39;everrrrrrr&#39;, &#39;at&#39;, &#39;Monticello&#39;, &#39;.&#39;, &#39;Awesommmmmmeeeeeeee&#39;, &#39;day&#39;, &#39;:*)&#39;] *************************************************** best approach to tokens: [&#39;RT&#39;, &#39;Best&#39;, &#39;day&#39;, &#39;everrr&#39;, &#39;at&#39;, &#39;Monticello&#39;, &#39;.&#39;, &#39;Awesommmeee&#39;, &#39;day&#39;, &#39;:*)&#39;] . n-grams . from nltk.util import ngrams print(&#39;list of tuples:&#39;, list(ngrams(tokens, 2))) print(&#39;*****************************************************&#39;) print(&#39;list of triplets:&#39;, list(ngrams(tokens, 3))) print(&#39;*****************************************************&#39;) print(&#39;list of 2-grams&#39;, [&#39; &#39;.join(x) for x in list(ngrams(tokens, 2))]) . list of tuples: [(&#39;Thomas&#39;, &#39;Jefferson&#39;), (&#39;Jefferson&#39;, &#39;began&#39;), (&#39;began&#39;, &#39;building&#39;), (&#39;building&#39;, &#39;Monticello&#39;), (&#39;Monticello&#39;, &#39;at&#39;), (&#39;at&#39;, &#39;the&#39;), (&#39;the&#39;, &#39;age&#39;), (&#39;age&#39;, &#39;of&#39;), (&#39;of&#39;, &#39;26&#39;)] ***************************************************** list of triplets: [(&#39;Thomas&#39;, &#39;Jefferson&#39;, &#39;began&#39;), (&#39;Jefferson&#39;, &#39;began&#39;, &#39;building&#39;), (&#39;began&#39;, &#39;building&#39;, &#39;Monticello&#39;), (&#39;building&#39;, &#39;Monticello&#39;, &#39;at&#39;), (&#39;Monticello&#39;, &#39;at&#39;, &#39;the&#39;), (&#39;at&#39;, &#39;the&#39;, &#39;age&#39;), (&#39;the&#39;, &#39;age&#39;, &#39;of&#39;), (&#39;age&#39;, &#39;of&#39;, &#39;26&#39;)] ***************************************************** list of 2-grams [&#39;Thomas Jefferson&#39;, &#39;Jefferson began&#39;, &#39;began building&#39;, &#39;building Monticello&#39;, &#39;Monticello at&#39;, &#39;at the&#39;, &#39;the age&#39;, &#39;age of&#39;, &#39;of 26&#39;] . Stopwords . stop_words = [&#39;a&#39;, &#39;an&#39;, &#39;the&#39;, &#39;on&#39;, &#39;of&#39;, &#39;off&#39;, &#39;this&#39;, &#39;is&#39;] tokens = [&#39;the&#39;, &#39;house&#39;, &#39;is&#39;, &#39;on&#39;, &#39;fire&#39;] tokens_without_stopwords = [x for x in tokens if x not in stop_words] print(&#39;tokens without stopwords:&#39;, tokens_without_stopwords) # canonical stopwords import nltk #nltk.download(&#39;stopwords&#39;) stop_words = nltk.corpus.stopwords.words(&#39;english&#39;) # print(&#39;size of nltk stopwords:&#39;, len(stop_words)) print(&#39;first seven words&#39;, stop_words[:7]) # [&#39;i&#39;, &#39;me&#39;, &#39;my&#39;, &#39;myself&#39;, &#39;we&#39;, &#39;our&#39;, &#39;ours&#39;] print(&#39;stopwords with 1 character in nltk:&#39;, [sw for sw in stop_words if len(sw) == 1]) # [&#39;i&#39;, &#39;a&#39;, &#39;s&#39;, &#39;t&#39;, &#39;d&#39;, &#39;m&#39;, &#39;o&#39;, &#39;y&#39;] # stopwords comparison between sklearn and NLTK from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as sklearn_stop_words nltk_stop_words = nltk.corpus.stopwords.words(&#39;english&#39;) print(&#39;nltk stopwords:&#39;, len(nltk_stop_words)) print(&#39;sklearn stopwords:&#39;, len(sklearn_stop_words)) #print(len(nltk_stop_words.union(sklearn_stop_words))) #print(len(nltk_stop_words.intersection(sklearn_stop_words))) . tokens without stopwords: [&#39;house&#39;, &#39;fire&#39;] first seven words [&#39;i&#39;, &#39;me&#39;, &#39;my&#39;, &#39;myself&#39;, &#39;we&#39;, &#39;our&#39;, &#39;ours&#39;] stopwords with 1 character in nltk: [&#39;i&#39;, &#39;a&#39;, &#39;s&#39;, &#39;t&#39;, &#39;d&#39;, &#39;m&#39;, &#39;o&#39;, &#39;y&#39;] nltk stopwords: 179 sklearn stopwords: 318 . Normalizing capitalization . tokens = [&#39;House&#39;, &#39;Visitor&#39;, &#39;Center&#39;] print([x.lower() for x in tokens]) . [&#39;house&#39;, &#39;visitor&#39;, &#39;center&#39;] . Stemming . stemmer with regular expressions . def stem(phrase): return &#39; &#39;.join([re.findall(&#39;^(.*ss|.*?)(s)?$&#39;, word)[0][0].strip(&quot;&#39;&quot;) for word in phrase.lower().split()]) print(stem(&#39;houses&#39;)) print(stem(&quot;Doctor House&#39;s calls&quot;)) # doctor house call . house doctor house call . complete stemmer . from nltk.stem.porter import PorterStemmer stemmer = PorterStemmer() &#39; &#39;.join([stemmer.stem(w).strip(&quot;&#39;&quot;) for w in &quot;dish washer&#39;s wash dishes&quot;.split()]) # dish washer wash dish . &#39;dish washer wash dish&#39; . Lemmatization . from nltk.stem import WordNetLemmatizer #nltk.download(&#39;wordnet&#39;) lemmatizer = WordNetLemmatizer() # if post isn&#39;t specified the lemmatizer assumes it is a noun print(lemmatizer.lemmatize(&#39;better&#39;)) print(lemmatizer.lemmatize(&#39;better&#39;, pos=&#39;a&#39;)) print(lemmatizer.lemmatize(&#39;good&#39;, pos=&#39;a&#39;)) print(lemmatizer.lemmatize(&#39;goods&#39;, pos=&#39;a&#39;)) print(lemmatizer.lemmatize(&#39;goods&#39;, pos=&#39;n&#39;)) print(lemmatizer.lemmatize(&#39;goodness&#39;, pos=&#39;n&#39;)) print(lemmatizer.lemmatize(&#39;best&#39;, pos=&#39;a&#39;)) . better good good goods good goodness best . Sentiment analysis . vader model for sentiment analysis . from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer sa = SentimentIntensityAnalyzer() text1 = &quot;Python is very readable and it is great for NLP.&quot; text2 = &quot;Python is not a bad choice for most applications.&quot; print(&#39;lexicon words with spaces&#39;, [(tok, score) for tok, score in sa.lexicon.items() if &quot; &quot; in tok]) print(&#39;*******************************************&#39;) print(&#39;dict sentiment for text1:&#39;, sa.polarity_scores(text=text1)) print(&#39;dict sentiment for text2:&#39;, sa.polarity_scores(text=text2)) . lexicon words with spaces [(&#34;( &#39;}{&#39; )&#34;, 1.6), (&#34;can&#39;t stand&#34;, -2.0), (&#39;fed up&#39;, -1.8), (&#39;screwed up&#39;, -1.5)] ******************************************* dict sentiment for text1: {&#39;neg&#39;: 0.0, &#39;neu&#39;: 0.687, &#39;pos&#39;: 0.313, &#39;compound&#39;: 0.6249} dict sentiment for text2: {&#39;neg&#39;: 0.0, &#39;neu&#39;: 0.737, &#39;pos&#39;: 0.263, &#39;compound&#39;: 0.431} . sentiment score for a given corpus . corpus = [&quot;Absolutely perfect! Love it! :-) :-) :-)&quot;, &quot;Horrible! Completely useless. :(&quot;, &quot;It was Ok. Some good and some bad things&quot;] for doc in corpus: scores = sa.polarity_scores(doc) print(&#39;{:+}: {}&#39;.format(scores[&#39;compound&#39;], doc)) . +0.9428: Absolutely perfect! Love it! :-) :-) :-) -0.8768: Horrible! Completely useless. :( -0.1531: It was Ok. Some good and some bad things . can&#39;t install nlpia . #from nlpia.data.loaders import get_data #movies = get_data(&#39;hutto_movies&#39;) #movies.head().round(2) # sentiment text # id # 1 2.27 The Rock is destined to be the 21st Century... # 2 3.53 The gorgeously elaborate continuation of &#39;&#39;... # 3 -0.60 Effective but too tepid ... # 4 1.47 If you sometimes like to go to the movies t... # 5 1.73 Emerges as something rare, an issue movie t... #movies.describe().round(2) # sentiment # count 10605.00 # mean 0.00 # min -3.88 # max 3.94 . A complete path for predicting sentiment analysis . generate sentiment from movie data. Remember you can&#39;t install nlpia where the data movie is . import pandas as pd import numpy as np reviews_train = [] for line in open(&#39;movie_data/full_train.txt&#39;, &#39;r&#39;): reviews_train.append(line.strip()) movies = pd.DataFrame(reviews_train, columns=[&#39;review&#39;]) movies[&#39;sentiment&#39;] = np.random.uniform(-4, 4, 25000).round(2) # sample the data movies = movies.sample(n=1000).reset_index(drop=True) . convert &#39;review&#39; column to bag of words . import pandas as pd from nltk.tokenize import casual_tokenize from collections import Counter pd.set_option(&#39;display.width&#39;, 75) bags_of_words = [] # tokenize each row, append into a list of dicts and convert to dataframe for text in movies.review.to_list(): bags_of_words.append(Counter(casual_tokenize(text))) df_bows = pd.DataFrame.from_records(bags_of_words) df_bows = df_bows.fillna(0).astype(int) print(&#39;bows shape:&#39;, df_bows.shape) print(&#39;**************************************************&#39;) print(&#39;first review:&#39;, movies.loc[0].review) print(&#39;**************************************************&#39;) print(&#39;tokens of the first review that appear in the first 5 rows of the dataset&#39;, df_bows.head()[list(bags_of_words[0].keys())]) . bows shape: (1000, 22298) ************************************************** first review: I&#39;ll give writer/director William Gove credit for finding someone to finance this ill-conceived &#34;thriller.&#34; A good argument for not wasting money subscribing to HBO, let alone buying DVDs based on cover art and blurbs. A pedestrian Dennis Hopper and a game Richard Grieco add nothing significant to their resumes, although the art direction is not half bad. The dialogue will leave you grimacing with wonder at its conceit; this is storytelling at its worst. No tension, no suspense, no dread, no fear, no empathy, no catharsis, no nothing. A few attractive and often nude females spice up the boredom, but this is definitely a film best seen as a trailer. I feel sorry for the guy who greenlighted this thing. Good for late-night, zoned-out viewing only. You have been warned. ************************************************** tokens of the first review that appear in the first 5 rows of the dataset I&#39;ll give writer / director William Gove credit for finding 0 1 1 1 1 1 1 1 1 4 1 1 0 0 0 12 0 0 0 0 2 0 2 0 0 0 0 0 0 0 0 1 0 3 0 0 0 6 1 0 0 0 3 0 4 0 0 0 4 0 0 0 0 2 0 ... thing Good late-night zoned-out viewing only You have 0 ... 1 1 1 1 1 1 1 1 1 ... 0 0 0 0 0 0 0 2 2 ... 1 0 0 0 0 1 0 1 3 ... 0 0 0 0 0 0 0 0 4 ... 0 0 0 0 0 2 0 0 been warned 0 1 1 1 0 0 2 0 0 3 0 0 4 0 0 [5 rows x 108 columns] . predict the sentiment and computing metrics for accuracy . from sklearn.naive_bayes import MultinomialNB nb = MultinomialNB() # fit the model and convert continuous to boolean label nb = nb.fit(df_bows, movies.sentiment &gt; 0) # values from review columns goes from -4 to 4 so this code normalize to the &quot;ground true&quot; sentiment movies[&#39;predicted_sentiment&#39;] = nb.predict_proba(df_bows)[:,1] * 8 - 4 movies[&#39;error&#39;] = (movies.predicted_sentiment - movies.sentiment).abs() # metrics print(&#39;MAE:&#39;, movies.error.mean()) # support columns movies[&#39;sentiment_ispositive&#39;] = (movies.sentiment &gt; 0).astype(int) movies[&#39;predicted_ispositive&#39;] = (movies.predicted_sentiment &gt; 0).astype(int) movies[&#39;&#39;&#39;sentiment predicted_sentiment sentiment_ispositive predicted_ispositive&#39;&#39;&#39;.split()].head(8) # sentiment predicted_sentiment sentiment_ispositive predicted_ispositive # id # 1 2.266667 4 1 1 # 2 3.533333 4 1 1 # 3 -0.600000 -4 0 0 # 4 1.466667 4 1 1 # 5 1.733333 4 1 1 # 6 2.533333 4 1 1 # 7 2.466667 4 1 1 # 8 1.266667 -4 1 0 # prediction over positives (movies.predicted_ispositive == movies.sentiment_ispositive).sum() / len(movies) . MAE: 2.0512326689917173 . 0.993 . nb.predict_proba( ) .",
            "url": "https://ccalobeto.github.io/nlp2/fastpages/jupyter/2021/02/13/nlp-in-action-i.html",
            "relUrl": "/fastpages/jupyter/2021/02/13/nlp-in-action-i.html",
            "date": " • Feb 13, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://ccalobeto.github.io/nlp2/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://ccalobeto.github.io/nlp2/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://ccalobeto.github.io/nlp2/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ccalobeto.github.io/nlp2/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}