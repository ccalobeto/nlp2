{
  
    
        "post0": {
            "title": "NLP in Action I",
            "content": "System tools . Install a pip package in the current Jupyter kernel . import sys # !{sys.executable} -m pip install vaderSentiment . Text Tools . Regular Expressions . import re . basics of regular expressions . [] indicates character class | r&#39;[ s]&#39; is equivalent to r&#39; t n r x0b x0c&#39; match all the spaces, tabs, returns, new lines and form-feed | r&#39;[a-z]&#39; match all lowercase | r&#39;[0-9]&#39; match any digit | r&#39;[_a-zA-Z]&#39; match any underscore character or letter of the english alphabet | . Match any sentence that begins with hi|hello|hey followed by space(s) and a word . r = &quot;(hi|hello|hey)[ ]*([a-z]*)&quot; . print(re.match(r, &#39;Hello Rosa&#39;, flags=re.IGNORECASE)) print(re.match(r, &quot;hi ho, hi ho, it&#39;s off to work ...&quot;, flags=re.IGNORECASE)) print(re.match(r, &quot;hey, what&#39;s up&quot;, flags=re.IGNORECASE)) . &lt;re.Match object; span=(0, 10), match=&#39;Hello Rosa&#39;&gt; None &lt;re.Match object; span=(0, 9), match=&#39;hey, what&#39;&gt; . Example with a complex pattern . r = r&quot;[^a-z]*([y]o|[h&#39;]?ello|ok|hey|(good[ ])?(morn[gin&#39;]{0,3}|&quot; r&quot;afternoon|even[gin&#39;]{0,3}))[ s,;:]{1,3}([a-z]{1,20})&quot; re_greeting = re.compile(r, flags=re.IGNORECASE) . print(re_greeting.match(&#39;Hello Rosa&#39;)) print(re_greeting.match(&#39;Hello Rosa&#39;).groups()) print(re_greeting.match(&quot;Good morning Rosa&quot;)) print(re_greeting.match(&quot;Good Manning Rosa&quot;)) print(re_greeting.match(&#39;Good evening Rosa Parks&#39;).groups()) print(re_greeting.match(&quot;Good Morn&#39;n Rosa&quot;)) print(re_greeting.match(&quot;yo Rosa&quot;)) . &lt;re.Match object; span=(0, 10), match=&#39;Hello Rosa&#39;&gt; (&#39;Hello&#39;, None, None, &#39;Rosa&#39;) &lt;re.Match object; span=(0, 17), match=&#39;Good morning Rosa&#39;&gt; None (&#39;Good evening&#39;, &#39;Good &#39;, &#39;evening&#39;, &#39;Rosa&#39;) &lt;re.Match object; span=(0, 16), match=&#34;Good Morn&#39;n Rosa&#34;&gt; &lt;re.Match object; span=(0, 7), match=&#39;yo Rosa&#39;&gt; . Simple Chat Bot . Enter the text &#39;good morning rosa or hello rose&#39; . my_names = set([&#39;rosa&#39;, &#39;rose&#39;, &#39;chatty&#39;, &#39;chatbot&#39;, &#39;bot&#39;, &#39;chatterbot&#39;]) curt_names = set([&#39;hal&#39;, &#39;you&#39;, &#39;u&#39;]) greeter_name = &#39;&#39; match = re_greeting.match(input()) if match: at_name = match.groups()[-1] if at_name in curt_names: print(&quot;Good one.&quot;) elif at_name.lower() in my_names: print(&quot;Hi {}, How are you?&quot;.format(greeter_name)) . Hi , How are you? . Word Permutations . n=3 permutations with text &#39;Good morning Rosa!&#39; . from itertools import permutations print([&quot; &quot;.join(combo) for combo in permutations(&quot;Good morning Rosa!&quot;.split(), 3)]) . [&#39;Good morning Rosa!&#39;, &#39;Good Rosa! morning&#39;, &#39;morning Good Rosa!&#39;, &#39;morning Rosa! Good&#39;, &#39;Rosa! Good morning&#39;, &#39;Rosa! morning Good&#39;] . Count Words . Counting words using Counter. Dict output . from collections import Counter print(Counter(&quot;Guten Morgen Rosa&quot;.split())) print(Counter(&quot;Good morning morning , Rosa!&quot;.split())) . Counter({&#39;Guten&#39;: 1, &#39;Morgen&#39;: 1, &#39;Rosa&#39;: 1}) Counter({&#39;morning&#39;: 2, &#39;Good&#39;: 1, &#39;,&#39;: 1, &#39;Rosa!&#39;: 1}) . Word Tokenization . Tokens . split into tokens . sentence = &quot;Thomas Jefferson began building Monticello at age of 26.&quot; sentence.split() str.split(sentence) . [&#39;Thomas&#39;, &#39;Jefferson&#39;, &#39;began&#39;, &#39;building&#39;, &#39;Monticello&#39;, &#39;at&#39;, &#39;age&#39;, &#39;of&#39;, &#39;26.&#39;] . split a sentence into tokens, order it and convert to a vector . import numpy as np token_sequence = str.split(sentence) vocab = sorted(set(token_sequence)) num_tokens = len(token_sequence) vocab_size = len(vocab) onehot_vectors = np.zeros((num_tokens, vocab_size), int) for i, word in enumerate(token_sequence): onehot_vectors[i, vocab.index(word)] = 1 print(&#39;, &#39;.join(vocab)) print(&#39;*********************&#39;) print(onehot_vectors) . 26., Jefferson, Monticello, Thomas, age, at, began, building, of ********************* [[0 0 0 1 0 0 0 0 0] [0 1 0 0 0 0 0 0 0] [0 0 0 0 0 0 1 0 0] [0 0 0 0 0 0 0 1 0] [0 0 1 0 0 0 0 0 0] [0 0 0 0 0 1 0 0 0] [0 0 0 0 1 0 0 0 0] [0 0 0 0 0 0 0 0 1] [1 0 0 0 0 0 0 0 0]] . Building a dataframe with matrix of vectors . import pandas as pd print(pd.DataFrame(onehot_vectors, columns=vocab)) . 26. Jefferson Monticello Thomas age at began building of 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 2 0 0 0 0 0 0 1 0 0 3 0 0 0 0 0 0 0 1 0 4 0 0 1 0 0 0 0 0 0 5 0 0 0 0 0 1 0 0 0 6 0 0 0 0 1 0 0 0 0 7 0 0 0 0 0 0 0 0 1 8 1 0 0 0 0 0 0 0 0 . construct a frequency vector . sentences = &quot;&quot;&quot;Thomas Jefferson began building Monticello at the age of 26. n&quot;&quot;&quot; sentences += &quot;&quot;&quot;Construction was done mostly by local masons and carpenters. n&quot;&quot;&quot; sentences += &quot;He moved into the South Pavilion in 1770. n&quot; sentences += &quot;&quot;&quot;Turning Monticello into a neoclassical masterpiece was Jefferson&#39;s obsession.&quot;&quot;&quot; print(&#39;text to analize:&#39;, sentences) print(&#39;*********************************************************&#39;) #1. construct a dict of dicts corpus = {} for i, sent in enumerate(sentences.split(&#39; n&#39;)): corpus[&#39;sent{}&#39;.format(i)] = dict((tok, 1) for tok in sent.split()) print(&#39;first dict:&#39;, corpus[&#39;sent0&#39;]) print(&#39;*********************************************************&#39;) #2. convert dict to dataframe df = pd.DataFrame.from_records(corpus).fillna(0).astype(int).T print(df[df.columns[:10]]) # Thomas Jefferson began building Monticello at the age of 26. # sent0 1 1 1 1 1 1 1 1 1 1 # sent1 0 0 0 0 0 0 0 0 0 0 # sent2 0 0 0 0 0 0 1 0 0 0 # sent3 0 0 0 0 1 0 0 0 0 0 print(&#39;**********************************************************&#39;) print(&#39;word shared by two sentences:&#39;,[(k, v) for (k, v) in (df.loc[&#39;sent0&#39;] &amp; df.loc[&#39;sent3&#39;]). items() if v]) . text to analize: Thomas Jefferson began building Monticello at the age of 26. Construction was done mostly by local masons and carpenters. He moved into the South Pavilion in 1770. Turning Monticello into a neoclassical masterpiece was Jefferson&#39;s obsession. ********************************************************* first dict: {&#39;Thomas&#39;: 1, &#39;Jefferson&#39;: 1, &#39;began&#39;: 1, &#39;building&#39;: 1, &#39;Monticello&#39;: 1, &#39;at&#39;: 1, &#39;the&#39;: 1, &#39;age&#39;: 1, &#39;of&#39;: 1, &#39;26.&#39;: 1} ********************************************************* Thomas Jefferson began building Monticello at the age of 26. sent0 1 1 1 1 1 1 1 1 1 1 sent1 0 0 0 0 0 0 0 0 0 0 sent2 0 0 0 0 0 0 1 0 0 0 sent3 0 0 0 0 1 0 0 0 0 0 ********************************************************** word shared by two sentences: [(&#39;Monticello&#39;, 1)] . split into tokens using regular expressions . import re sentence = &quot;&quot;&quot;Thomas Jefferson began building Monticello at the age of 26.&quot;&quot;&quot; tokens = re.split(r&#39;[- s.,;!?]+&#39;, sentence) print(&#39;list of tokens:&#39;, tokens) print(&#39;***********************************************&#39;) # a better version of tokenization fast and manageable pattern = re.compile(r&quot;([- s.,;!?])+&quot;) tokens = pattern.split(sentence) print(&#39;last 10 tokens:&#39;, tokens[-10:]) # [&#39;the&#39;, &#39; &#39;, &#39;age&#39;, &#39; &#39;, &#39;of&#39;, &#39; &#39;, &#39;26&#39;, &#39;.&#39;, &#39;&#39;] . list of tokens: [&#39;Thomas&#39;, &#39;Jefferson&#39;, &#39;began&#39;, &#39;building&#39;, &#39;Monticello&#39;, &#39;at&#39;, &#39;the&#39;, &#39;age&#39;, &#39;of&#39;, &#39;26&#39;, &#39;&#39;] *********************************************** last 10 tokens: [&#39; &#39;, &#39;the&#39;, &#39; &#39;, &#39;age&#39;, &#39; &#39;, &#39;of&#39;, &#39; &#39;, &#39;26&#39;, &#39;.&#39;, &#39;&#39;] . ignoring punctuation . print(&#39;removing punctuation:&#39;, [x for x in tokens if x not in &#39;- t n.,;!?&#39;]) . removing punctuation: [&#39;Thomas&#39;, &#39;Jefferson&#39;, &#39;began&#39;, &#39;building&#39;, &#39;Monticello&#39;, &#39;at&#39;, &#39;the&#39;, &#39;age&#39;, &#39;of&#39;, &#39;26&#39;] . ignoring whitespaces with tokeneizer . from nltk.tokenize import RegexpTokenizer tokenizer = RegexpTokenizer(r&#39; w+|$[0-9.]+| S+&#39;) print(tokenizer.tokenize(sentence)) . [&#39;Thomas&#39;, &#39;Jefferson&#39;, &#39;began&#39;, &#39;building&#39;, &#39;Monticello&#39;, &#39;at&#39;, &#39;the&#39;, &#39;age&#39;, &#39;of&#39;, &#39;26&#39;, &#39;.&#39;] . managing contractions with tokeneizer . from nltk.tokenize import TreebankWordTokenizer sentence = &quot;&quot;&quot;Monticello wasn&#39;t designated as UNESCO World Heritage Site until 1987.&quot;&quot;&quot; tokenizer = TreebankWordTokenizer() print(tokenizer.tokenize(sentence)) . [&#39;Monticello&#39;, &#39;was&#39;, &#34;n&#39;t&#34;, &#39;designated&#39;, &#39;as&#39;, &#39;UNESCO&#39;, &#39;World&#39;, &#39;Heritage&#39;, &#39;Site&#39;, &#39;until&#39;, &#39;1987&#39;, &#39;.&#39;] . tokenize informal conversation . from nltk.tokenize.casual import casual_tokenize message = &quot;&quot;&quot;RT @TJMonticello Best day everrrrrrr at Monticello. Awesommmmmmeeeeeeee day :*)&quot;&quot;&quot; print(&#39;tokens:&#39;, casual_tokenize(message)) print(&#39;***************************************************&#39;) print(&#39;best approach to tokens:&#39;, casual_tokenize(message, reduce_len=True, strip_handles=True)) . tokens: [&#39;RT&#39;, &#39;@TJMonticello&#39;, &#39;Best&#39;, &#39;day&#39;, &#39;everrrrrrr&#39;, &#39;at&#39;, &#39;Monticello&#39;, &#39;.&#39;, &#39;Awesommmmmmeeeeeeee&#39;, &#39;day&#39;, &#39;:*)&#39;] *************************************************** best approach to tokens: [&#39;RT&#39;, &#39;Best&#39;, &#39;day&#39;, &#39;everrr&#39;, &#39;at&#39;, &#39;Monticello&#39;, &#39;.&#39;, &#39;Awesommmeee&#39;, &#39;day&#39;, &#39;:*)&#39;] . n-grams . from nltk.util import ngrams print(&#39;list of tuples:&#39;, list(ngrams(tokens, 2))) print(&#39;*****************************************************&#39;) print(&#39;list of triplets:&#39;, list(ngrams(tokens, 3))) print(&#39;*****************************************************&#39;) print(&#39;list of 2-grams&#39;, [&#39; &#39;.join(x) for x in list(ngrams(tokens, 2))]) . list of tuples: [(&#39;Thomas&#39;, &#39;Jefferson&#39;), (&#39;Jefferson&#39;, &#39;began&#39;), (&#39;began&#39;, &#39;building&#39;), (&#39;building&#39;, &#39;Monticello&#39;), (&#39;Monticello&#39;, &#39;at&#39;), (&#39;at&#39;, &#39;the&#39;), (&#39;the&#39;, &#39;age&#39;), (&#39;age&#39;, &#39;of&#39;), (&#39;of&#39;, &#39;26&#39;)] ***************************************************** list of triplets: [(&#39;Thomas&#39;, &#39;Jefferson&#39;, &#39;began&#39;), (&#39;Jefferson&#39;, &#39;began&#39;, &#39;building&#39;), (&#39;began&#39;, &#39;building&#39;, &#39;Monticello&#39;), (&#39;building&#39;, &#39;Monticello&#39;, &#39;at&#39;), (&#39;Monticello&#39;, &#39;at&#39;, &#39;the&#39;), (&#39;at&#39;, &#39;the&#39;, &#39;age&#39;), (&#39;the&#39;, &#39;age&#39;, &#39;of&#39;), (&#39;age&#39;, &#39;of&#39;, &#39;26&#39;)] ***************************************************** list of 2-grams [&#39;Thomas Jefferson&#39;, &#39;Jefferson began&#39;, &#39;began building&#39;, &#39;building Monticello&#39;, &#39;Monticello at&#39;, &#39;at the&#39;, &#39;the age&#39;, &#39;age of&#39;, &#39;of 26&#39;] . Stopwords . stop_words = [&#39;a&#39;, &#39;an&#39;, &#39;the&#39;, &#39;on&#39;, &#39;of&#39;, &#39;off&#39;, &#39;this&#39;, &#39;is&#39;] tokens = [&#39;the&#39;, &#39;house&#39;, &#39;is&#39;, &#39;on&#39;, &#39;fire&#39;] tokens_without_stopwords = [x for x in tokens if x not in stop_words] print(&#39;tokens without stopwords:&#39;, tokens_without_stopwords) # canonical stopwords import nltk #nltk.download(&#39;stopwords&#39;) stop_words = nltk.corpus.stopwords.words(&#39;english&#39;) # print(&#39;size of nltk stopwords:&#39;, len(stop_words)) print(&#39;first seven words&#39;, stop_words[:7]) # [&#39;i&#39;, &#39;me&#39;, &#39;my&#39;, &#39;myself&#39;, &#39;we&#39;, &#39;our&#39;, &#39;ours&#39;] print(&#39;stopwords with 1 character in nltk:&#39;, [sw for sw in stop_words if len(sw) == 1]) # [&#39;i&#39;, &#39;a&#39;, &#39;s&#39;, &#39;t&#39;, &#39;d&#39;, &#39;m&#39;, &#39;o&#39;, &#39;y&#39;] # stopwords comparison between sklearn and NLTK from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as sklearn_stop_words nltk_stop_words = nltk.corpus.stopwords.words(&#39;english&#39;) print(&#39;nltk stopwords:&#39;, len(nltk_stop_words)) print(&#39;sklearn stopwords:&#39;, len(sklearn_stop_words)) #print(len(nltk_stop_words.union(sklearn_stop_words))) #print(len(nltk_stop_words.intersection(sklearn_stop_words))) . tokens without stopwords: [&#39;house&#39;, &#39;fire&#39;] first seven words [&#39;i&#39;, &#39;me&#39;, &#39;my&#39;, &#39;myself&#39;, &#39;we&#39;, &#39;our&#39;, &#39;ours&#39;] stopwords with 1 character in nltk: [&#39;i&#39;, &#39;a&#39;, &#39;s&#39;, &#39;t&#39;, &#39;d&#39;, &#39;m&#39;, &#39;o&#39;, &#39;y&#39;] nltk stopwords: 179 sklearn stopwords: 318 . Normalizing capitalization . tokens = [&#39;House&#39;, &#39;Visitor&#39;, &#39;Center&#39;] print([x.lower() for x in tokens]) . [&#39;house&#39;, &#39;visitor&#39;, &#39;center&#39;] . Stemming . stemmer with regular expressions . def stem(phrase): return &#39; &#39;.join([re.findall(&#39;^(.*ss|.*?)(s)?$&#39;, word)[0][0].strip(&quot;&#39;&quot;) for word in phrase.lower().split()]) print(stem(&#39;houses&#39;)) print(stem(&quot;Doctor House&#39;s calls&quot;)) # doctor house call . house doctor house call . complete stemmer . from nltk.stem.porter import PorterStemmer stemmer = PorterStemmer() &#39; &#39;.join([stemmer.stem(w).strip(&quot;&#39;&quot;) for w in &quot;dish washer&#39;s wash dishes&quot;.split()]) # dish washer wash dish . &#39;dish washer wash dish&#39; . Lemmatization . from nltk.stem import WordNetLemmatizer #nltk.download(&#39;wordnet&#39;) lemmatizer = WordNetLemmatizer() # if post isn&#39;t specified the lemmatizer assumes it is a noun print(lemmatizer.lemmatize(&#39;better&#39;)) print(lemmatizer.lemmatize(&#39;better&#39;, pos=&#39;a&#39;)) print(lemmatizer.lemmatize(&#39;good&#39;, pos=&#39;a&#39;)) print(lemmatizer.lemmatize(&#39;goods&#39;, pos=&#39;a&#39;)) print(lemmatizer.lemmatize(&#39;goods&#39;, pos=&#39;n&#39;)) print(lemmatizer.lemmatize(&#39;goodness&#39;, pos=&#39;n&#39;)) print(lemmatizer.lemmatize(&#39;best&#39;, pos=&#39;a&#39;)) . better good good goods good goodness best . Sentiment analysis . vader model for sentiment analysis . from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer sa = SentimentIntensityAnalyzer() text1 = &quot;Python is very readable and it is great for NLP.&quot; text2 = &quot;Python is not a bad choice for most applications.&quot; print(&#39;lexicon words with spaces&#39;, [(tok, score) for tok, score in sa.lexicon.items() if &quot; &quot; in tok]) print(&#39;*******************************************&#39;) print(&#39;dict sentiment for text1:&#39;, sa.polarity_scores(text=text1)) print(&#39;dict sentiment for text2:&#39;, sa.polarity_scores(text=text2)) . lexicon words with spaces [(&#34;( &#39;}{&#39; )&#34;, 1.6), (&#34;can&#39;t stand&#34;, -2.0), (&#39;fed up&#39;, -1.8), (&#39;screwed up&#39;, -1.5)] ******************************************* dict sentiment for text1: {&#39;neg&#39;: 0.0, &#39;neu&#39;: 0.687, &#39;pos&#39;: 0.313, &#39;compound&#39;: 0.6249} dict sentiment for text2: {&#39;neg&#39;: 0.0, &#39;neu&#39;: 0.737, &#39;pos&#39;: 0.263, &#39;compound&#39;: 0.431} . sentiment score for a given corpus . corpus = [&quot;Absolutely perfect! Love it! :-) :-) :-)&quot;, &quot;Horrible! Completely useless. :(&quot;, &quot;It was Ok. Some good and some bad things&quot;] for doc in corpus: scores = sa.polarity_scores(doc) print(&#39;{:+}: {}&#39;.format(scores[&#39;compound&#39;], doc)) . +0.9428: Absolutely perfect! Love it! :-) :-) :-) -0.8768: Horrible! Completely useless. :( -0.1531: It was Ok. Some good and some bad things . naive model for sentiment analysis . code to use movies data with nlpia but i can&#39;t install it . #from nlpia.data.loaders import get_data #movies = get_data(&#39;hutto_movies&#39;) #movies.head().round(2) . generate sentiment from movie data. Remember you can&#39;t install nlpia where the data movie is . import pandas as pd import numpy as np reviews_train = [] for line in open(&#39;movie_data/full_train.txt&#39;, &#39;r&#39;): reviews_train.append(line.strip()) movies = pd.DataFrame(reviews_train, columns=[&#39;review&#39;]) movies[&#39;sentiment&#39;] = np.random.uniform(-4, 4, 25000).round(2) # sample the data movies = movies.sample(n=1000).reset_index(drop=True) movies . review sentiment . 0 Woman (Miriam Hopkins as Virginia) chases Man ... | 3.49 | . 1 The combination of reading the Novella and vie... | -3.81 | . 2 When my now college age daughter was in presch... | 2.21 | . 3 Absolutely awful movie. Utter waste of time.&lt;b... | -0.94 | . 4 I disagree with previous comment about this mo... | 0.14 | . ... ... | ... | . 995 It&#39;s rare that I feel a need to write a review... | -0.86 | . 996 Regardless of what personal opinion one may ha... | 0.90 | . 997 I have recently seen this movie due to Jake&#39;s ... | 0.18 | . 998 There is absolutely no plot in this movie ...n... | -2.02 | . 999 After gorging myself on a variety of seemingly... | -1.87 | . 1000 rows × 2 columns . sentiment goes from -4 to 4 . movies.describe() . sentiment . count 1000.000000 | . mean -0.015730 | . std 2.362015 | . min -3.990000 | . 25% -2.055000 | . 50% -0.145000 | . 75% 2.180000 | . max 4.000000 | . convert &#39;review&#39; column to bag of words . import pandas as pd from nltk.tokenize import casual_tokenize from collections import Counter pd.set_option(&#39;display.width&#39;, 75) bags_of_words = [] # tokenize each row, append into a list of dicts and convert to dataframe for text in movies.review.to_list(): bags_of_words.append(Counter(casual_tokenize(text))) df_bows = pd.DataFrame.from_records(bags_of_words) df_bows = df_bows.fillna(0).astype(int) print(&#39;bows shape:&#39;, df_bows.shape) print(&#39;**************************************************&#39;) print(&#39;first review:&#39;) print(movies.loc[0].review) print(&#39;**************************************************&#39;) print(&#39;108 tokens of the first review, with a lexicon of 22298 tokens&#39;) df_bows.head()[list(bags_of_words[0].keys())] . bows shape: (1000, 22298) ************************************************** first review: I&#39;ll give writer/director William Gove credit for finding someone to finance this ill-conceived &#34;thriller.&#34; A good argument for not wasting money subscribing to HBO, let alone buying DVDs based on cover art and blurbs. A pedestrian Dennis Hopper and a game Richard Grieco add nothing significant to their resumes, although the art direction is not half bad. The dialogue will leave you grimacing with wonder at its conceit; this is storytelling at its worst. No tension, no suspense, no dread, no fear, no empathy, no catharsis, no nothing. A few attractive and often nude females spice up the boredom, but this is definitely a film best seen as a trailer. I feel sorry for the guy who greenlighted this thing. Good for late-night, zoned-out viewing only. You have been warned. ************************************************** 108 tokens of the first review, with a lexicon of 22298 tokens . I&#39;ll give writer / director William Gove credit for finding ... thing Good late-night zoned-out viewing only You have been warned . 0 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 4 | 1 | ... | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | . 1 0 | 0 | 0 | 12 | 0 | 0 | 0 | 0 | 2 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2 | 0 | 0 | . 2 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | ... | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | . 3 0 | 0 | 0 | 6 | 1 | 0 | 0 | 0 | 3 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 4 0 | 0 | 0 | 4 | 0 | 0 | 0 | 0 | 2 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 2 | 0 | 0 | 0 | 0 | . 5 rows × 108 columns . predict the sentiment and computing metrics for accuracy . from sklearn.naive_bayes import MultinomialNB nb = MultinomialNB() # fit the model and convert continuous to boolean label nb = nb.fit(df_bows, movies.sentiment &gt; 0) # values from review columns goes from -4 to 4 so this code normalize to the &quot;ground true&quot; # sentiment. predict_proba return a n x 2 ndarray, we use el &quot;1s&quot; column movies[&#39;predicted_sentiment&#39;] = nb.predict_proba(df_bows)[:,1] * 8 - 4 movies[&#39;error&#39;] = (movies.predicted_sentiment - movies.sentiment).abs() # metrics print(&#39;MAE:&#39;, movies.error.mean()) # support columns movies[&#39;sentiment_ispositive&#39;] = (movies.sentiment &gt; 0).astype(int) movies[&#39;predicted_ispositive&#39;] = (movies.predicted_sentiment &gt; 0).astype(int) movies[&#39;&#39;&#39;sentiment predicted_sentiment sentiment_ispositive predicted_ispositive&#39;&#39;&#39;.split()].head(8) # prediction over positives print(&#39;positives accuracy:&#39;, (movies.predicted_ispositive == movies.sentiment_ispositive).sum() / len(movies)) . MAE: 2.0512326689917173 positives accuracy: 0.993 . Math with words . Term Frequency . from nltk.tokenize import TreebankWordTokenizer from collections import Counter sentence = &quot;&quot;&quot;The faster Harry got to the store, the faster Harry, the faster, would get home.&quot;&quot;&quot; tokenizer = TreebankWordTokenizer() tokens = tokenizer.tokenize(sentence.lower()) bags_of_words = Counter(tokens) bags_of_words.most_common(4) # top 4 bag of words bags_of_words_df = pd.DataFrame.from_records([bags_of_words]) # put the tokens on columns, give the form of many sentences tf = (bags_of_words_df / len(bags_of_words)).round(2) tf . the faster harry got to store , would get home . . 0 0.36 | 0.27 | 0.18 | 0.09 | 0.09 | 0.09 | 0.27 | 0.09 | 0.09 | 0.09 | 0.09 | . Bag Of Words . Counting . from collections import Counter from nltk.tokenize import TreebankWordTokenizer tokenizer = TreebankWordTokenizer() # van&#39;t install nlpia but from nlpia.data.loaders import kite_text kite_text = &quot;&quot;&quot;A kite is traditionally a tethered heavier-than-air craft with wing surfaces that react against the air to create lift and drag. A kite consists of wings, tethers, and anchors. Kites often have a bridle to guide the face of the kite at the correct angle so the wind can lift it. A kite’s wing also may be so designed so a bridle is not needed; when kiting a sailplane for launch, the tether meets the wing at a single point. A kite may have fixed or moving anchors. Untraditionally in technical kiting, a kite consists of tether-set-coupled wing sets; even in technical kiting, though, a wing in the system is still often called the kite. The lift that sustains the kite in flight is generated when air flows around the kite’s surface, producing low pressure above and high pressure below the wings. The interaction with the wind also generates horizontal drag along the direction of the wind. The resultant force vector from the lift and drag force components is opposed by the tension of one or more of the lines or tethers to which the kite is attached. The anchor point of the kite line may be static or moving (such as the towing of a kite by a running person, boat, free-falling anchors as in paragliders and fugitive parakites or vehicle). The same principles of fluid flow apply in liquids and kites are also used under water. A hybrid tethered craft comprising both a lighter-than-air balloon as well as a kite lifting surface is called a kytoon. Kites have a long and varied history and many different types are flown individually and at festivals worldwide. Kites may be flown for recreation, art or other practical uses. Sport kites can be flown in aerial ballet, sometimes as part of a competition. Power kites are multi-line steerable kites designed to generate large forces which can be used to power activities such as kite surfing, kite landboarding, kite fishing, kite buggying and a new trend snow kiting. Even Man-lifting kites have been made. &quot;&quot;&quot; tokens = tokenizer.tokenize(kite_text.lower()) token_counts = Counter(tokens) . token_counts . . Counter({&#39;a&#39;: 20, &#39;kite&#39;: 14, &#39;is&#39;: 7, &#39;traditionally&#39;: 1, &#39;tethered&#39;: 2, &#39;heavier-than-air&#39;: 1, &#39;craft&#39;: 2, &#39;with&#39;: 2, &#39;wing&#39;: 5, &#39;surfaces&#39;: 1, &#39;that&#39;: 2, &#39;react&#39;: 1, &#39;against&#39;: 1, &#39;the&#39;: 26, &#39;air&#39;: 2, &#39;to&#39;: 5, &#39;create&#39;: 1, &#39;lift&#39;: 4, &#39;and&#39;: 10, &#39;drag.&#39;: 1, &#39;consists&#39;: 2, &#39;of&#39;: 10, &#39;wings&#39;: 1, &#39;,&#39;: 14, &#39;tethers&#39;: 2, &#39;anchors.&#39;: 2, &#39;kites&#39;: 8, &#39;often&#39;: 2, &#39;have&#39;: 4, &#39;bridle&#39;: 2, &#39;guide&#39;: 1, &#39;face&#39;: 1, &#39;at&#39;: 3, &#39;correct&#39;: 1, &#39;angle&#39;: 1, &#39;so&#39;: 3, &#39;wind&#39;: 2, &#39;can&#39;: 3, &#39;it.&#39;: 1, &#39;kite’s&#39;: 2, &#39;also&#39;: 3, &#39;may&#39;: 4, &#39;be&#39;: 5, &#39;designed&#39;: 2, &#39;not&#39;: 1, &#39;needed&#39;: 1, &#39;;&#39;: 2, &#39;when&#39;: 2, &#39;kiting&#39;: 3, &#39;sailplane&#39;: 1, &#39;for&#39;: 2, &#39;launch&#39;: 1, &#39;tether&#39;: 1, &#39;meets&#39;: 1, &#39;single&#39;: 1, &#39;point.&#39;: 1, &#39;fixed&#39;: 1, &#39;or&#39;: 6, &#39;moving&#39;: 2, &#39;untraditionally&#39;: 1, &#39;in&#39;: 7, &#39;technical&#39;: 2, &#39;tether-set-coupled&#39;: 1, &#39;sets&#39;: 1, &#39;even&#39;: 2, &#39;though&#39;: 1, &#39;system&#39;: 1, &#39;still&#39;: 1, &#39;called&#39;: 2, &#39;kite.&#39;: 1, &#39;sustains&#39;: 1, &#39;flight&#39;: 1, &#39;generated&#39;: 1, &#39;flows&#39;: 1, &#39;around&#39;: 1, &#39;surface&#39;: 2, &#39;producing&#39;: 1, &#39;low&#39;: 1, &#39;pressure&#39;: 2, &#39;above&#39;: 1, &#39;high&#39;: 1, &#39;below&#39;: 1, &#39;wings.&#39;: 1, &#39;interaction&#39;: 1, &#39;generates&#39;: 1, &#39;horizontal&#39;: 1, &#39;drag&#39;: 2, &#39;along&#39;: 1, &#39;direction&#39;: 1, &#39;wind.&#39;: 1, &#39;resultant&#39;: 1, &#39;force&#39;: 2, &#39;vector&#39;: 1, &#39;from&#39;: 1, &#39;components&#39;: 1, &#39;opposed&#39;: 1, &#39;by&#39;: 2, &#39;tension&#39;: 1, &#39;one&#39;: 1, &#39;more&#39;: 1, &#39;lines&#39;: 1, &#39;which&#39;: 2, &#39;attached.&#39;: 1, &#39;anchor&#39;: 1, &#39;point&#39;: 1, &#39;line&#39;: 1, &#39;static&#39;: 1, &#39;(&#39;: 1, &#39;such&#39;: 2, &#39;as&#39;: 6, &#39;towing&#39;: 1, &#39;running&#39;: 1, &#39;person&#39;: 1, &#39;boat&#39;: 1, &#39;free-falling&#39;: 1, &#39;anchors&#39;: 1, &#39;paragliders&#39;: 1, &#39;fugitive&#39;: 1, &#39;parakites&#39;: 1, &#39;vehicle&#39;: 1, &#39;)&#39;: 1, &#39;.&#39;: 2, &#39;same&#39;: 1, &#39;principles&#39;: 1, &#39;fluid&#39;: 1, &#39;flow&#39;: 1, &#39;apply&#39;: 1, &#39;liquids&#39;: 1, &#39;are&#39;: 3, &#39;used&#39;: 2, &#39;under&#39;: 1, &#39;water.&#39;: 1, &#39;hybrid&#39;: 1, &#39;comprising&#39;: 1, &#39;both&#39;: 1, &#39;lighter-than-air&#39;: 1, &#39;balloon&#39;: 1, &#39;well&#39;: 1, &#39;lifting&#39;: 1, &#39;kytoon.&#39;: 1, &#39;long&#39;: 1, &#39;varied&#39;: 1, &#39;history&#39;: 1, &#39;many&#39;: 1, &#39;different&#39;: 1, &#39;types&#39;: 1, &#39;flown&#39;: 3, &#39;individually&#39;: 1, &#39;festivals&#39;: 1, &#39;worldwide.&#39;: 1, &#39;recreation&#39;: 1, &#39;art&#39;: 1, &#39;other&#39;: 1, &#39;practical&#39;: 1, &#39;uses.&#39;: 1, &#39;sport&#39;: 1, &#39;aerial&#39;: 1, &#39;ballet&#39;: 1, &#39;sometimes&#39;: 1, &#39;part&#39;: 1, &#39;competition.&#39;: 1, &#39;power&#39;: 2, &#39;multi-line&#39;: 1, &#39;steerable&#39;: 1, &#39;generate&#39;: 1, &#39;large&#39;: 1, &#39;forces&#39;: 1, &#39;activities&#39;: 1, &#39;surfing&#39;: 1, &#39;landboarding&#39;: 1, &#39;fishing&#39;: 1, &#39;buggying&#39;: 1, &#39;new&#39;: 1, &#39;trend&#39;: 1, &#39;snow&#39;: 1, &#39;kiting.&#39;: 1, &#39;man-lifting&#39;: 1, &#39;been&#39;: 1, &#39;made&#39;: 1}) . Removing stopwords . import nltk # nltk.download(&#39;stopwords&#39;, quiet=True) stopwords = nltk.corpus.stopwords.words(&#39;english&#39;) tokens = [x for x in tokens if x not in stopwords] kite_counts = Counter(tokens) . kite_counts . . Counter({&#39;kite&#39;: 14, &#39;traditionally&#39;: 1, &#39;tethered&#39;: 2, &#39;heavier-than-air&#39;: 1, &#39;craft&#39;: 2, &#39;wing&#39;: 5, &#39;surfaces&#39;: 1, &#39;react&#39;: 1, &#39;air&#39;: 2, &#39;create&#39;: 1, &#39;lift&#39;: 4, &#39;drag.&#39;: 1, &#39;consists&#39;: 2, &#39;wings&#39;: 1, &#39;,&#39;: 14, &#39;tethers&#39;: 2, &#39;anchors.&#39;: 2, &#39;kites&#39;: 8, &#39;often&#39;: 2, &#39;bridle&#39;: 2, &#39;guide&#39;: 1, &#39;face&#39;: 1, &#39;correct&#39;: 1, &#39;angle&#39;: 1, &#39;wind&#39;: 2, &#39;it.&#39;: 1, &#39;kite’s&#39;: 2, &#39;also&#39;: 3, &#39;may&#39;: 4, &#39;designed&#39;: 2, &#39;needed&#39;: 1, &#39;;&#39;: 2, &#39;kiting&#39;: 3, &#39;sailplane&#39;: 1, &#39;launch&#39;: 1, &#39;tether&#39;: 1, &#39;meets&#39;: 1, &#39;single&#39;: 1, &#39;point.&#39;: 1, &#39;fixed&#39;: 1, &#39;moving&#39;: 2, &#39;untraditionally&#39;: 1, &#39;technical&#39;: 2, &#39;tether-set-coupled&#39;: 1, &#39;sets&#39;: 1, &#39;even&#39;: 2, &#39;though&#39;: 1, &#39;system&#39;: 1, &#39;still&#39;: 1, &#39;called&#39;: 2, &#39;kite.&#39;: 1, &#39;sustains&#39;: 1, &#39;flight&#39;: 1, &#39;generated&#39;: 1, &#39;flows&#39;: 1, &#39;around&#39;: 1, &#39;surface&#39;: 2, &#39;producing&#39;: 1, &#39;low&#39;: 1, &#39;pressure&#39;: 2, &#39;high&#39;: 1, &#39;wings.&#39;: 1, &#39;interaction&#39;: 1, &#39;generates&#39;: 1, &#39;horizontal&#39;: 1, &#39;drag&#39;: 2, &#39;along&#39;: 1, &#39;direction&#39;: 1, &#39;wind.&#39;: 1, &#39;resultant&#39;: 1, &#39;force&#39;: 2, &#39;vector&#39;: 1, &#39;components&#39;: 1, &#39;opposed&#39;: 1, &#39;tension&#39;: 1, &#39;one&#39;: 1, &#39;lines&#39;: 1, &#39;attached.&#39;: 1, &#39;anchor&#39;: 1, &#39;point&#39;: 1, &#39;line&#39;: 1, &#39;static&#39;: 1, &#39;(&#39;: 1, &#39;towing&#39;: 1, &#39;running&#39;: 1, &#39;person&#39;: 1, &#39;boat&#39;: 1, &#39;free-falling&#39;: 1, &#39;anchors&#39;: 1, &#39;paragliders&#39;: 1, &#39;fugitive&#39;: 1, &#39;parakites&#39;: 1, &#39;vehicle&#39;: 1, &#39;)&#39;: 1, &#39;.&#39;: 2, &#39;principles&#39;: 1, &#39;fluid&#39;: 1, &#39;flow&#39;: 1, &#39;apply&#39;: 1, &#39;liquids&#39;: 1, &#39;used&#39;: 2, &#39;water.&#39;: 1, &#39;hybrid&#39;: 1, &#39;comprising&#39;: 1, &#39;lighter-than-air&#39;: 1, &#39;balloon&#39;: 1, &#39;well&#39;: 1, &#39;lifting&#39;: 1, &#39;kytoon.&#39;: 1, &#39;long&#39;: 1, &#39;varied&#39;: 1, &#39;history&#39;: 1, &#39;many&#39;: 1, &#39;different&#39;: 1, &#39;types&#39;: 1, &#39;flown&#39;: 3, &#39;individually&#39;: 1, &#39;festivals&#39;: 1, &#39;worldwide.&#39;: 1, &#39;recreation&#39;: 1, &#39;art&#39;: 1, &#39;practical&#39;: 1, &#39;uses.&#39;: 1, &#39;sport&#39;: 1, &#39;aerial&#39;: 1, &#39;ballet&#39;: 1, &#39;sometimes&#39;: 1, &#39;part&#39;: 1, &#39;competition.&#39;: 1, &#39;power&#39;: 2, &#39;multi-line&#39;: 1, &#39;steerable&#39;: 1, &#39;generate&#39;: 1, &#39;large&#39;: 1, &#39;forces&#39;: 1, &#39;activities&#39;: 1, &#39;surfing&#39;: 1, &#39;landboarding&#39;: 1, &#39;fishing&#39;: 1, &#39;buggying&#39;: 1, &#39;new&#39;: 1, &#39;trend&#39;: 1, &#39;snow&#39;: 1, &#39;kiting.&#39;: 1, &#39;man-lifting&#39;: 1, &#39;made&#39;: 1}) . Vectorizing the text . document_vector = [] doc_length = len(tokens) for key, value in kite_counts.most_common(): document_vector.append(value / doc_length) print(&#39;Printing the first 10 values of the list&#39;) document_vector[:10] . Printing the first 10 values of the list . [1.75, 1.75, 1.0, 0.625, 0.5, 0.5, 0.375, 0.375, 0.375, 0.25] . Lexicon . Lexicon with a new corpus . docs = [&quot;The faster Harry got to the score, the faster and faster Harry would get home.&quot;] docs.append(&quot;Harry is hairy and faster than Jill.&quot;) docs.append(&quot;Jill is not as hairy as Harry.&quot;) doc_tokens = [] for doc in docs: doc_tokens += [sorted(tokenizer.tokenize(doc.lower()))] print(&#39;number of tokens of the first term in the list:&#39;, len(doc_tokens[0])) all_doc_tokens = sum(doc_tokens, []) print(&#39;number of total tokens:&#39;, len(all_doc_tokens)) lexicon = sorted(set(all_doc_tokens)) print(&#39;**************************************************&#39;) print(&#39;size of the lexicon:&#39;, len(lexicon)) print(&#39;lexicon:&#39;, lexicon) . number of tokens of the first term in the list: 17 number of total tokens: 33 ************************************************** size of the lexicon: 18 lexicon: [&#39;,&#39;, &#39;.&#39;, &#39;and&#39;, &#39;as&#39;, &#39;faster&#39;, &#39;get&#39;, &#39;got&#39;, &#39;hairy&#39;, &#39;harry&#39;, &#39;home&#39;, &#39;is&#39;, &#39;jill&#39;, &#39;not&#39;, &#39;score&#39;, &#39;than&#39;, &#39;the&#39;, &#39;to&#39;, &#39;would&#39;] . vectorize the sentences in a list of OrderedDict . from collections import OrderedDict zero_vector = OrderedDict((token, 0) for token in lexicon) import copy doc_vectors = [] for doc in docs: vec = copy.copy(zero_vector) tokens = tokenizer.tokenize(doc.lower()) token_counts = Counter(tokens) for key, value in token_counts.items(): vec[key] = value / len(lexicon) doc_vectors.append(vec) print(&#39;Printint the First OrderedDict that corresponds to the first sentence:&#39;) doc_vectors[2] . Printint the First OrderedDict that corresponds to the first sentence: . OrderedDict([(&#39;,&#39;, 0), (&#39;.&#39;, 0.05555555555555555), (&#39;and&#39;, 0), (&#39;as&#39;, 0.1111111111111111), (&#39;faster&#39;, 0), (&#39;get&#39;, 0), (&#39;got&#39;, 0), (&#39;hairy&#39;, 0.05555555555555555), (&#39;harry&#39;, 0.05555555555555555), (&#39;home&#39;, 0), (&#39;is&#39;, 0.05555555555555555), (&#39;jill&#39;, 0.05555555555555555), (&#39;not&#39;, 0.05555555555555555), (&#39;score&#39;, 0), (&#39;than&#39;, 0), (&#39;the&#39;, 0), (&#39;to&#39;, 0), (&#39;would&#39;, 0)]) . CosineSimilarity . import math def cosine_sim(vec1, vec2): &quot;&quot;&quot; Let&#39;s convert our dictionaries to lists for easier matching.&quot;&quot;&quot; vec1 = [val for val in vec1.values()] vec2 = [val for val in vec2.values()] dot_prod = 0 for i, v in enumerate(vec1): dot_prod += v * vec2[i] mag_1 = math.sqrt(sum([x**2 for x in vec1])) mag_2 = math.sqrt(sum([x**2 for x in vec2])) return dot_prod / (mag_1 * mag_2) . TF-IDF . 1st approach TF-IDF, the harder version . kite_history = &quot;&quot;&quot;Kites were invented in China, where materials ideal for kite building were readily available: silk fabric for sail material; fine, high-tensile-strength silk for flying line; and resilient bamboo for a strong, lightweight framework. The kite has been claimed as the invention of the 5th-century BC Chinese philosophers Mozi (also Mo Di) and Lu Ban (also Gongshu Ban). By 549 AD paper kites were certainly being flown, as it was recorded that in that year a paper kite was used as a message for a rescue mission. Ancient and medieval Chinese sources describe kites being used for measuring distances, testing the wind, lifting men, signaling, and communication for military operations. The earliest known Chinese kites were flat (not bowed) and often rectangular. Later, tailless kites incorporated a stabilizing bowline. Kites were decorated with mythological motifs and legendary figures; some were fitted with strings and whistles to make musical sounds while flying. From China, kites were introduced to Cambodia, Thailand, India, Japan, Korea and the western world. After its introduction into India, the kite further evolved into the fighter kite, known as the patang in India, where thousands are flown every year on festivals such as Makar Sankranti. Kites were known throughout Polynesia, as far as New Zealand, with the assumption being that the knowledge diffused from China along with the people. Anthropomorphic kites made from cloth and wood were used in religious ceremonies to send prayers to the gods. Polynesian kite traditions are used by anthropologists get an idea of early “primitive” Asian traditions that are believed to have at one time existed in Asia.&quot;&quot;&quot; . detect a list of tokens from texts . intro_tokens = tokenizer.tokenize(kite_text.lower()) history_tokens = tokenizer.tokenize(kite_history.lower()) intro_total = len(intro_tokens) history_total = len(history_tokens) print(&#39;tokens in kite text:&#39;, intro_total) print(&#39;tokens in kite history:&#39;, history_total) print(&#39;&#39;) print(&quot;This section is to show what zipf&#39;s law is&quot;) # compute the TF of one word in two documents intro_tf = {} history_tf = {} intro_counts = Counter(intro_tokens) intro_tf[&#39;kite&#39;] = intro_counts[&#39;kite&#39;] / intro_total history_counts = Counter(history_tokens) history_tf[&#39;kite&#39;] = history_counts[&#39;kite&#39;] / history_total # the tf of word &quot;kite&quot; in document A is twice times the tf of word kite in document B # (according to zipf&#39;s law) print(&#39;Term Frequency of word &quot;kite&quot; in intro document is: {:.4f}&#39;.format(intro_tf[&#39;kite&#39;])) print(&#39;Term Frequency of word &quot;kite&quot; in history document is: {:.4f}&#39;.format(history_tf[&#39;kite&#39;])) # But zipf&#39;s law is not fulfilled with word &quot;and&quot; intro_tf[&#39;and&#39;] = intro_counts[&#39;and&#39;] / intro_total history_tf[&#39;and&#39;] = history_counts[&#39;and&#39;] / history_total print(&#39;Term Frequency of word &quot;and&quot; in intro document is: {:.4f}&#39;.format(intro_tf[&#39;and&#39;])) print(&#39;Term Frequency of word &quot;and&quot; in history document is: {:.4f}&#39;.format(history_tf[&#39;and&#39;])) print(&#39;&#39;) . tokens in kite text: 361 tokens in kite history: 295 This section is to show what zipf&#39;s law is Term Frequency of word &#34;kite&#34; in intro document is: 0.0388 Term Frequency of word &#34;kite&#34; in history document is: 0.0203 Term Frequency of word &#34;and&#34; in intro document is: 0.0277 Term Frequency of word &#34;and&#34; in history document is: 0.0305 . calculating the TF-IDF of only 3 words: &quot;and&quot;, &quot;kite&quot; and &quot;china&quot; . num_docs_containing_and = 0 for doc in [intro_tokens, history_tokens]: if &#39;and&#39; in doc: num_docs_containing_and += 1 num_docs_containing_kite = 0 for doc in [intro_tokens, history_tokens]: if &#39;and&#39; in doc: num_docs_containing_kite += 1 num_docs_containing_china = 0 for doc in [intro_tokens, history_tokens]: if &#39;and&#39; in doc: num_docs_containing_china += 1 intro_tf[&#39;china&#39;] = intro_counts[&#39;china&#39;] / intro_total history_tf[&#39;china&#39;] = history_counts[&#39;china&#39;] / history_total num_docs = 2 intro_idf = {} history_idf = {} intro_idf[&#39;and&#39;] = num_docs / num_docs_containing_and history_idf[&#39;and&#39;] = num_docs / num_docs_containing_and intro_idf[&#39;kite&#39;] = num_docs / num_docs_containing_kite history_idf[&#39;kite&#39;] = num_docs / num_docs_containing_kite intro_idf[&#39;china&#39;] = num_docs / num_docs_containing_china history_idf[&#39;china&#39;] = num_docs / num_docs_containing_china intro_tfidf = {} intro_tfidf[&#39;and&#39;] = intro_tf[&#39;and&#39;] * intro_idf[&#39;and&#39;] intro_tfidf[&#39;kite&#39;] = intro_tf[&#39;kite&#39;] * intro_idf[&#39;kite&#39;] intro_tfidf[&#39;china&#39;] = intro_tf[&#39;china&#39;] * intro_idf[&#39;china&#39;] history_tfidf = {} history_tfidf[&#39;and&#39;] = history_tf[&#39;and&#39;] * history_idf[&#39;and&#39;] history_tfidf[&#39;kite&#39;] = history_tf[&#39;kite&#39;] * history_idf[&#39;kite&#39;] history_tfidf[&#39;china&#39;] = history_tf[&#39;china&#39;] * history_idf[&#39;china&#39;] print(&#39;TF-IDF of the words in document intro is: &#39;, intro_tfidf) print(&#39;TF-IDF of the words in documenty history is &#39;,history_tfidf) . TF-IDF of the words in document intro is: {&#39;and&#39;: 0.027700831024930747, &#39;kite&#39;: 0.038781163434903045, &#39;china&#39;: 0.0} TF-IDF of the words in documenty history is {&#39;and&#39;: 0.030508474576271188, &#39;kite&#39;: 0.020338983050847456, &#39;china&#39;: 0.010169491525423728} . 2nd approach TF-IDF with similarities. . converting a list of text into a list of tuples containing the token and its count . import copy print(&#39;corpus:&#39;, docs) print(&#39;&#39;) document_tfidf_vectors = [] for doc in docs: vec = copy.copy(zero_vector) tokens = tokenizer.tokenize(doc.lower()) token_counts = Counter(tokens) for key, value in token_counts.items(): docs_containing_key = 0 for _doc in docs: if key in _doc: docs_containing_key += 1 tf = value / len(lexicon) if docs_containing_key: idf = len(docs) / docs_containing_key else: idf = 0 vec[key] = tf * idf document_tfidf_vectors.append(vec) document_tfidf_vectors . corpus: [&#39;The faster Harry got to the score, the faster and faster Harry would get home.&#39;, &#39;Harry is hairy and faster than Jill.&#39;, &#39;Jill is not as hairy as Harry.&#39;] . [OrderedDict([(&#39;,&#39;, 0.16666666666666666), (&#39;.&#39;, 0.05555555555555555), (&#39;and&#39;, 0.08333333333333333), (&#39;as&#39;, 0), (&#39;faster&#39;, 0.25), (&#39;get&#39;, 0.16666666666666666), (&#39;got&#39;, 0.16666666666666666), (&#39;hairy&#39;, 0), (&#39;harry&#39;, 0.0), (&#39;home&#39;, 0.16666666666666666), (&#39;is&#39;, 0), (&#39;jill&#39;, 0), (&#39;not&#39;, 0), (&#39;score&#39;, 0.16666666666666666), (&#39;than&#39;, 0), (&#39;the&#39;, 0.5), (&#39;to&#39;, 0.16666666666666666), (&#39;would&#39;, 0.16666666666666666)]), OrderedDict([(&#39;,&#39;, 0), (&#39;.&#39;, 0.05555555555555555), (&#39;and&#39;, 0.08333333333333333), (&#39;as&#39;, 0), (&#39;faster&#39;, 0.08333333333333333), (&#39;get&#39;, 0), (&#39;got&#39;, 0), (&#39;hairy&#39;, 0.08333333333333333), (&#39;harry&#39;, 0.0), (&#39;home&#39;, 0), (&#39;is&#39;, 0.08333333333333333), (&#39;jill&#39;, 0.0), (&#39;not&#39;, 0), (&#39;score&#39;, 0), (&#39;than&#39;, 0.16666666666666666), (&#39;the&#39;, 0), (&#39;to&#39;, 0), (&#39;would&#39;, 0)]), OrderedDict([(&#39;,&#39;, 0), (&#39;.&#39;, 0.05555555555555555), (&#39;and&#39;, 0), (&#39;as&#39;, 0.1111111111111111), (&#39;faster&#39;, 0), (&#39;get&#39;, 0), (&#39;got&#39;, 0), (&#39;hairy&#39;, 0.08333333333333333), (&#39;harry&#39;, 0.0), (&#39;home&#39;, 0), (&#39;is&#39;, 0.08333333333333333), (&#39;jill&#39;, 0.0), (&#39;not&#39;, 0.16666666666666666), (&#39;score&#39;, 0), (&#39;than&#39;, 0), (&#39;the&#39;, 0), (&#39;to&#39;, 0), (&#39;would&#39;, 0)])] . query = &quot;How long does it take to get to the store?&quot; query_vec = copy.copy(zero_vector) # normalize the query tokens = tokenizer.tokenize(query.lower()) # a dic with tokens with counts token_counts = Counter(tokens) documents = docs for key, value in token_counts.items(): docs_containing_key = 0 for _doc in documents: if key in _doc.lower(): docs_containing_key += 1 # Avoiding divide-by-zero-error if docs_containing_key == 0: continue tf = value / len(tokens) idf = len(documents) / docs_containing_key query_vec[key] = tf * idf print(&#39;TF-IDF of query(list version):&#39;,query_vec) . TF-IDF of query(list version): OrderedDict([(&#39;,&#39;, 0), (&#39;.&#39;, 0), (&#39;and&#39;, 0), (&#39;as&#39;, 0), (&#39;faster&#39;, 0), (&#39;get&#39;, 0.2727272727272727), (&#39;got&#39;, 0), (&#39;hairy&#39;, 0), (&#39;harry&#39;, 0), (&#39;home&#39;, 0), (&#39;is&#39;, 0), (&#39;jill&#39;, 0), (&#39;not&#39;, 0), (&#39;score&#39;, 0), (&#39;than&#39;, 0), (&#39;the&#39;, 0.2727272727272727), (&#39;to&#39;, 0.5454545454545454), (&#39;would&#39;, 0)]) . computing similarities between documents . print(&#39;similarities between query and 1st, 2nd and 3rd document:&#39;) print(cosine_sim(query_vec, document_tfidf_vectors[0])) print(cosine_sim(query_vec, document_tfidf_vectors[1])) print(cosine_sim(query_vec, document_tfidf_vectors[2])) . similarities between query and 1st, 2nd and 3rd document: 0.5677922680888605 0.0 0.0 . 3rd Approach - The best version with sckit learn. . from sklearn.feature_extraction.text import TfidfVectorizer corpus = docs vectorizer = TfidfVectorizer(min_df=1) model = vectorizer.fit_transform(corpus) print(&#39;corpus ndarray output:&#39;) print(&#39;message: need to include the similarities(like 2nd approach)between&#39; +&#39;TF-IDF vectors: query and corpus!&#39;) model.todense().round(2) . corpus ndarray output: message: need to include the similarities (like 2nd approach)betweenTF-IDF vectors: query and corpus! . array([[0.16, 0. , 0.48, 0.21, 0.21, 0. , 0.25, 0.21, 0. , 0. , 0. , 0.21, 0. , 0.64, 0.21, 0.21], [0.37, 0. , 0.37, 0. , 0. , 0.37, 0.29, 0. , 0.37, 0.37, 0. , 0. , 0.49, 0. , 0. , 0. ], [0. , 0.75, 0. , 0. , 0. , 0.29, 0.22, 0. , 0.29, 0.29, 0.38, 0. , 0. , 0. , 0. , 0. ]]) . Finding Meanings in Words . Spam classifier . get the data . import pandas as pd from nlpia.data.loaders import get_data pd.options.display.width = 120 sms = get_data(&#39;sms-spam&#39;) index = [&#39;sms{}{}&#39;.format(i, &#39;!&#39;*j) for (i,j) in zip(range(len(sms)), sms.spam)] sms = pd.DataFrame(sms.values, columns=sms.columns, index=index) sms[&#39;spam&#39;] = sms.spam.astype(int) print(&#39;number of spam messages&#39;, len(sms)) sms.spam.sum() sms.head(6) . number of spam mails 4837 . spam text . sms0 0 | Go until jurong point, crazy.. Available only ... | . sms1 0 | Ok lar... Joking wif u oni... | . sms2! 1 | Free entry in 2 a wkly comp to win FA Cup fina... | . sms3 0 | U dun say so early hor... U c already then say... | . sms4 0 | Nah I don&#39;t think he goes to usf, he lives aro... | . sms5! 1 | FreeMsg Hey there darling it&#39;s been 3 week&#39;s n... | . TF-IDF transformation .",
            "url": "https://ccalobeto.github.io/nlp2/fastpages/jupyter/2021/02/13/nlp-in-action-i.html",
            "relUrl": "/fastpages/jupyter/2021/02/13/nlp-in-action-i.html",
            "date": " • Feb 13, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://ccalobeto.github.io/nlp2/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://ccalobeto.github.io/nlp2/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://ccalobeto.github.io/nlp2/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ccalobeto.github.io/nlp2/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}