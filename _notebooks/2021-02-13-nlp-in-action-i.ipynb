{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP in Action\n",
    "> My journey to learn NLP\n",
    "\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: false\n",
    "- comments: false\n",
    "- categories: [fastpages, jupyter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install a pip package in the current Jupyter kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### basics of regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [] indicates character class\n",
    "- r'[\\s]' is equivalent to r'\\t\\n\\r\\x0b\\x0c' match all the spaces, tabs, returns, new lines and \n",
    " form-feed\n",
    "- r'[a-z]' match all lowercase\n",
    "- r'[0-9]' match any digit\n",
    "- r'[_a-zA-Z]' match any underscore character or letter of the english alphabet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Match any sentence that begins with hi|hello|hey followed by space(s) and a word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = \"(hi|hello|hey)[ ]*([a-z]*)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 10), match='Hello Rosa'>\n",
      "None\n",
      "<re.Match object; span=(0, 9), match='hey, what'>\n"
     ]
    }
   ],
   "source": [
    "print(re.match(r, 'Hello Rosa', flags=re.IGNORECASE))\n",
    "print(re.match(r, \"hi ho, hi ho, it's off to work ...\", flags=re.IGNORECASE))\n",
    "print(re.match(r, \"hey, what's up\", flags=re.IGNORECASE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example with a complex pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = r\"[^a-z]*([y]o|[h']?ello|ok|hey|(good[ ])?(morn[gin']{0,3}|\"\\\n",
    "     r\"afternoon|even[gin']{0,3}))[\\s,;:]{1,3}([a-z]{1,20})\"\n",
    "\n",
    "re_greeting = re.compile(r, flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 10), match='Hello Rosa'>\n",
      "('Hello', None, None, 'Rosa')\n",
      "<re.Match object; span=(0, 17), match='Good morning Rosa'>\n",
      "None\n",
      "('Good evening', 'Good ', 'evening', 'Rosa')\n",
      "<re.Match object; span=(0, 16), match=\"Good Morn'n Rosa\">\n",
      "<re.Match object; span=(0, 7), match='yo Rosa'>\n"
     ]
    }
   ],
   "source": [
    "print(re_greeting.match('Hello Rosa'))\n",
    "print(re_greeting.match('Hello Rosa').groups())\n",
    "print(re_greeting.match(\"Good morning Rosa\"))\n",
    "print(re_greeting.match(\"Good Manning Rosa\"))\n",
    "print(re_greeting.match('Good evening Rosa Parks').groups())\n",
    "print(re_greeting.match(\"Good Morn'n Rosa\"))\n",
    "print(re_greeting.match(\"yo Rosa\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Chat Bot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter the text 'good morning rosa or hello rose'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " hello rose\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi , How are you?\n"
     ]
    }
   ],
   "source": [
    "my_names = set(['rosa', 'rose', 'chatty', 'chatbot', 'bot', 'chatterbot'])\n",
    "curt_names = set(['hal', 'you', 'u'])\n",
    "greeter_name = ''\n",
    "match = re_greeting.match(input())\n",
    "\n",
    "if match:\n",
    "    at_name = match.groups()[-1]\n",
    "    if at_name in curt_names:\n",
    "        print(\"Good one.\")\n",
    "\n",
    "    elif at_name.lower() in my_names:\n",
    "        print(\"Hi {}, How are you?\".format(greeter_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Permutations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n=3 permutations with text 'Good morning Rosa!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good morning Rosa!', 'Good Rosa! morning', 'morning Good Rosa!', 'morning Rosa! Good', 'Rosa! Good morning', 'Rosa! morning Good']\n"
     ]
    }
   ],
   "source": [
    "from itertools import permutations\n",
    "print([\" \".join(combo) for combo in permutations(\"Good morning Rosa!\".split(), 3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting words using Counter. Dict output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Guten': 1, 'Morgen': 1, 'Rosa': 1})\n",
      "Counter({'morning': 2, 'Good': 1, ',': 1, 'Rosa!': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(Counter(\"Guten Morgen Rosa\".split()))\n",
    "print(Counter(\"Good morning morning , Rosa!\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thomas',\n",
       " 'Jefferson',\n",
       " 'began',\n",
       " 'building',\n",
       " 'Monticello',\n",
       " 'at',\n",
       " 'age',\n",
       " 'of',\n",
       " '26.']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Thomas Jefferson began building Monticello at age of 26.\"\n",
    "sentence.split()\n",
    "str.split(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split a sentence into tokens, order it and convert to a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26., Jefferson, Monticello, Thomas, age, at, began, building, of\n",
      "*********************\n",
      "[[0 0 0 1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 1 0]\n",
      " [0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1]\n",
      " [1 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "token_sequence = str.split(sentence)\n",
    "vocab = sorted(set(token_sequence))\n",
    "num_tokens = len(token_sequence)\n",
    "vocab_size = len(vocab)\n",
    "onehot_vectors = np.zeros((num_tokens, vocab_size), int)\n",
    "\n",
    "for i, word in enumerate(token_sequence):\n",
    "    onehot_vectors[i, vocab.index(word)] = 1\n",
    "\n",
    "print(', '.join(vocab))\n",
    "print('*********************')\n",
    "print(onehot_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a dataframe with matrix of vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   26.  Jefferson  Monticello  Thomas  age  at  began  building  of\n",
      "0    0          0           0       1    0   0      0         0   0\n",
      "1    0          1           0       0    0   0      0         0   0\n",
      "2    0          0           0       0    0   0      1         0   0\n",
      "3    0          0           0       0    0   0      0         1   0\n",
      "4    0          0           1       0    0   0      0         0   0\n",
      "5    0          0           0       0    0   1      0         0   0\n",
      "6    0          0           0       0    1   0      0         0   0\n",
      "7    0          0           0       0    0   0      0         0   1\n",
      "8    1          0           0       0    0   0      0         0   0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(pd.DataFrame(onehot_vectors, columns=vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "construct a frequency vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text to analize: Thomas Jefferson began building Monticello at the age of 26.\n",
      "Construction was done mostly by local masons and carpenters.\n",
      "He moved into the South Pavilion in 1770.\n",
      "Turning Monticello into a neoclassical masterpiece was Jefferson's obsession.\n",
      "*********************************************************\n",
      "first dict: {'Thomas': 1, 'Jefferson': 1, 'began': 1, 'building': 1, 'Monticello': 1, 'at': 1, 'the': 1, 'age': 1, 'of': 1, '26.': 1}\n",
      "*********************************************************\n",
      "       Thomas  Jefferson  began  building  Monticello  at  the  age  of  26.\n",
      "sent0       1          1      1         1           1   1    1    1   1    1\n",
      "sent1       0          0      0         0           0   0    0    0   0    0\n",
      "sent2       0          0      0         0           0   0    1    0   0    0\n",
      "sent3       0          0      0         0           1   0    0    0   0    0\n",
      "**********************************************************\n",
      "word shared by two sentences: [('Monticello', 1)]\n"
     ]
    }
   ],
   "source": [
    "sentences = \"\"\"Thomas Jefferson began building Monticello at the age of 26.\\n\"\"\"\n",
    "sentences += \"\"\"Construction was done mostly by local masons and carpenters.\\n\"\"\"\n",
    "sentences += \"He moved into the South Pavilion in 1770.\\n\"\n",
    "sentences += \"\"\"Turning Monticello into a neoclassical masterpiece was Jefferson's obsession.\"\"\"\n",
    "\n",
    "print('text to analize:', sentences)\n",
    "print('*********************************************************')\n",
    "\n",
    "#1. construct a dict of dicts\n",
    "corpus = {}\n",
    "for i, sent in enumerate(sentences.split('\\n')):\n",
    "    corpus['sent{}'.format(i)] = dict((tok, 1) for tok in sent.split()) \n",
    "\n",
    "print('first dict:', corpus['sent0'])\n",
    "print('*********************************************************')\n",
    "#2. convert dict to dataframe \n",
    "df = pd.DataFrame.from_records(corpus).fillna(0).astype(int).T\n",
    "print(df[df.columns[:10]])\n",
    "#\t    Thomas\tJefferson\tbegan\tbuilding\tMonticello\tat\tthe\tage\tof\t26.\n",
    "# sent0\t    1\t    1\t    1\t        1\t        1\t    1\t1\t1\t1\t1\n",
    "# sent1\t    0\t    0\t    0\t        0\t        0\t    0\t0\t0\t0\t0\n",
    "# sent2\t    0\t    0\t    0\t        0\t        0\t    0\t1\t0\t0\t0\n",
    "# sent3\t    0\t    0\t    0\t        0\t        1\t    0\t0\t0\t0\t0\n",
    "\n",
    "print('**********************************************************')\n",
    "print('word shared by two sentences:',[(k, v) for (k, v) in (df.loc['sent0'] & df.loc['sent3']).\n",
    "                                       items() if v])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split into tokens using [regular expressions](#### basics of regular expressions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list of tokens: ['Thomas', 'Jefferson', 'began', 'building', 'Monticello', 'at', 'the', 'age', 'of', '26', '']\n",
      "***********************************************\n",
      "last 10 tokens: [' ', 'the', ' ', 'age', ' ', 'of', ' ', '26', '.', '']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "sentence = \"\"\"Thomas Jefferson began building Monticello at the age of 26.\"\"\"\n",
    "tokens = re.split(r'[-\\s.,;!?]+', sentence)\n",
    "print('list of tokens:', tokens)\n",
    "print('***********************************************')\n",
    "# a better version of tokenization fast and manageable\n",
    "pattern = re.compile(r\"([-\\s.,;!?])+\")\n",
    "tokens = pattern.split(sentence)\n",
    "print('last 10 tokens:', tokens[-10:])  \n",
    "# ['the', ' ', 'age', ' ', 'of', ' ', '26', '.', '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ignoring punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing punctuation: ['Thomas', 'Jefferson', 'began', 'building', 'Monticello', 'at', 'the', 'age', 'of', '26']\n"
     ]
    }
   ],
   "source": [
    "print('removing punctuation:', [x for x in tokens if x not in '- \\t\\n.,;!?'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ignoring whitespaces with tokeneizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Thomas', 'Jefferson', 'began', 'building', 'Monticello', 'at', 'the', 'age', 'of', '26', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+|$[0-9.]+|\\S+')\n",
    "print(tokenizer.tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### managing contractions with tokeneizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Monticello', 'was', \"n't\", 'designated', 'as', 'UNESCO', 'World', 'Heritage', 'Site', 'until', '1987', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "sentence = \"\"\"Monticello wasn't designated as UNESCO World Heritage Site until 1987.\"\"\"\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "print(tokenizer.tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tokenize informal conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: ['RT', '@TJMonticello', 'Best', 'day', 'everrrrrrr', 'at', 'Monticello', '.', 'Awesommmmmmeeeeeeee', 'day', ':*)']\n",
      "***************************************************\n",
      "best approach to tokens: ['RT', 'Best', 'day', 'everrr', 'at', 'Monticello', '.', 'Awesommmeee', 'day', ':*)']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize.casual import casual_tokenize\n",
    "message = \"\"\"RT @TJMonticello Best day everrrrrrr at Monticello. Awesommmmmmeeeeeeee day :*)\"\"\"\n",
    "print('tokens:', casual_tokenize(message))\n",
    "\n",
    "print('***************************************************')\n",
    "print('best approach to tokens:', casual_tokenize(message, reduce_len=True, strip_handles=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list of tuples: [('Thomas', 'Jefferson'), ('Jefferson', 'began'), ('began', 'building'), ('building', 'Monticello'), ('Monticello', 'at'), ('at', 'the'), ('the', 'age'), ('age', 'of'), ('of', '26')]\n",
      "*****************************************************\n",
      "list of triplets: [('Thomas', 'Jefferson', 'began'), ('Jefferson', 'began', 'building'), ('began', 'building', 'Monticello'), ('building', 'Monticello', 'at'), ('Monticello', 'at', 'the'), ('at', 'the', 'age'), ('the', 'age', 'of'), ('age', 'of', '26')]\n",
      "*****************************************************\n",
      "list of 2-grams ['Thomas Jefferson', 'Jefferson began', 'began building', 'building Monticello', 'Monticello at', 'at the', 'the age', 'age of', 'of 26']\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "print('list of tuples:', list(ngrams(tokens, 2)))\n",
    "print('*****************************************************')\n",
    "print('list of triplets:', list(ngrams(tokens, 3)))\n",
    "print('*****************************************************')\n",
    "print('list of 2-grams', [' '.join(x) for x in list(ngrams(tokens, 2))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens without stopwords: ['house', 'fire']\n",
      "first seven words ['i', 'me', 'my', 'myself', 'we', 'our', 'ours']\n",
      "stopwords with 1 character in nltk: ['i', 'a', 's', 't', 'd', 'm', 'o', 'y']\n",
      "nltk stopwords: 179\n",
      "sklearn stopwords: 318\n"
     ]
    }
   ],
   "source": [
    "# removing stopwords be careful, depends on the application\n",
    "stop_words = ['a', 'an', 'the', 'on', 'of', 'off', 'this', 'is']\n",
    "tokens = ['the', 'house', 'is', 'on', 'fire']\n",
    "tokens_without_stopwords = [x for x in tokens if x not in stop_words]\n",
    "print('tokens without stopwords:', tokens_without_stopwords)\n",
    "\n",
    "# canonical stopwords\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "# print('size of nltk stopwords:', len(stop_words))\n",
    "print('first seven words', stop_words[:7])\n",
    "# ['i', 'me', 'my', 'myself', 'we', 'our', 'ours']\n",
    "print('stopwords with 1 character in nltk:', [sw for sw in stop_words if len(sw) == 1])\n",
    "# ['i', 'a', 's', 't', 'd', 'm', 'o', 'y']\n",
    "\n",
    "# stopwords comparison between sklearn and NLTK\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as sklearn_stop_words\n",
    "nltk_stop_words = nltk.corpus.stopwords.words('english')\n",
    "print('nltk stopwords:', len(nltk_stop_words))\n",
    "print('sklearn stopwords:', len(sklearn_stop_words))\n",
    "\n",
    "#print(len(nltk_stop_words.union(sklearn_stop_words)))\n",
    "#print(len(nltk_stop_words.intersection(sklearn_stop_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing capitalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['house', 'visitor', 'center']\n"
     ]
    }
   ],
   "source": [
    "tokens = ['House', 'Visitor', 'Center']\n",
    "print([x.lower() for x in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### stemmer with regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "house\n",
      "doctor house call\n"
     ]
    }
   ],
   "source": [
    "def stem(phrase):\n",
    "    return ' '.join([re.findall('^(.*ss|.*?)(s)?$', word)[0][0].strip(\"'\") for word in phrase.lower().split()])\n",
    "\n",
    "print(stem('houses'))\n",
    "print(stem(\"Doctor House's calls\"))\n",
    "# doctor house call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### complete stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dish washer wash dish'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "' '.join([stemmer.stem(w).strip(\"'\") for w in \"dish washer's wash dishes\".split()])\n",
    "# dish washer wash dish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "better\n",
      "good\n",
      "good\n",
      "goods\n",
      "good\n",
      "goodness\n",
      "best\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# if post isn't specified the lemmatizer assumes it is a noun\n",
    "print(lemmatizer.lemmatize('better'))\n",
    "print(lemmatizer.lemmatize('better', pos='a'))\n",
    "print(lemmatizer.lemmatize('good', pos='a'))\n",
    "print(lemmatizer.lemmatize('goods', pos='a'))\n",
    "print(lemmatizer.lemmatize('goods', pos='n'))\n",
    "print(lemmatizer.lemmatize('goodness', pos='n'))\n",
    "print(lemmatizer.lemmatize('best', pos='a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vader model for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lexicon words with spaces [(\"( '}{' )\", 1.6), (\"can't stand\", -2.0), ('fed up', -1.8), ('screwed up', -1.5)]\n",
      "*******************************************\n",
      "dict sentiment for text1: {'neg': 0.0, 'neu': 0.687, 'pos': 0.313, 'compound': 0.6249}\n",
      "dict sentiment for text2: {'neg': 0.0, 'neu': 0.737, 'pos': 0.263, 'compound': 0.431}\n"
     ]
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sa = SentimentIntensityAnalyzer()\n",
    "text1 = \"Python is very readable and it is great for NLP.\"\n",
    "text2 = \"Python is not a bad choice for most applications.\"\n",
    "\n",
    "print('lexicon words with spaces', [(tok, score) for tok, score in sa.lexicon.items() if \" \" in tok])\n",
    "print('*******************************************')\n",
    "print('dict sentiment for text1:', sa.polarity_scores(text=text1))\n",
    "print('dict sentiment for text2:', sa.polarity_scores(text=text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sentiment score for a given corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+0.9428: Absolutely perfect! Love it! :-) :-) :-)\n",
      "-0.8768: Horrible! Completely useless. :(\n",
      "-0.1531: It was Ok. Some good and some bad things\n"
     ]
    }
   ],
   "source": [
    "corpus = [\"Absolutely perfect! Love it! :-) :-) :-)\",\n",
    "          \"Horrible! Completely useless. :(\",\n",
    "          \"It was Ok. Some good and some bad things\"]\n",
    "\n",
    "for doc in corpus:\n",
    "    scores = sa.polarity_scores(doc)\n",
    "    print('{:+}: {}'.format(scores['compound'], doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### can't install nlpia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive bayes model\n",
    "#from nlpia.data.loaders import get_data\n",
    "#movies = get_data('hutto_movies')\n",
    "#movies.head().round(2)\n",
    "#     sentiment                                            text\n",
    "# id\n",
    "# 1        2.27  The Rock is destined to be the 21st Century...\n",
    "# 2        3.53  The gorgeously elaborate continuation of ''...\n",
    "# 3       -0.60                     Effective but too tepid ...\n",
    "# 4        1.47  If you sometimes like to go to the movies t...\n",
    "# 5        1.73  Emerges as something rare, an issue movie t...\n",
    "#movies.describe().round(2)\n",
    "#        sentiment\n",
    "# count   10605.00\n",
    "# mean        0.00\n",
    "# min        -3.88\n",
    "# max         3.94"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A complete path for predicting sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate sentiment from movie data. Remember you can't install nlpia where the data movie is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "reviews_train = []\n",
    "for line in open('movie_data/full_train.txt', 'r'):\n",
    "    reviews_train.append(line.strip())\n",
    "movies = pd.DataFrame(reviews_train, columns=['review'])\n",
    "\n",
    "movies['sentiment'] = np.random.uniform(-4, 4, 25000).round(2)\n",
    "# sample the data\n",
    "movies = movies.sample(n=1000).reset_index(drop=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert 'review' column to bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bows shape: (1000, 22298)\n",
      "**************************************************\n",
      "first review: I'll give writer/director William Gove credit for finding someone to finance this ill-conceived \"thriller.\" A good argument for not wasting money subscribing to HBO, let alone buying DVDs based on cover art and blurbs. A pedestrian Dennis Hopper and a game Richard Grieco add nothing significant to their resumes, although the art direction is not half bad. The dialogue will leave you grimacing with wonder at its conceit; this is storytelling at its worst. No tension, no suspense, no dread, no fear, no empathy, no catharsis, no nothing. A few attractive and often nude females spice up the boredom, but this is definitely a film best seen as a trailer. I feel sorry for the guy who greenlighted this thing. Good for late-night, zoned-out viewing only. You have been warned.\n",
      "**************************************************\n",
      "tokens of the first review that appear in the first 5 rows of the dataset    I'll  give  writer   /  director  William  Gove  credit  for  finding  \\\n",
      "0     1     1       1   1         1        1     1       1    4        1   \n",
      "1     0     0       0  12         0        0     0       0    2        0   \n",
      "2     0     0       0   0         0        0     0       0    1        0   \n",
      "3     0     0       0   6         1        0     0       0    3        0   \n",
      "4     0     0       0   4         0        0     0       0    2        0   \n",
      "\n",
      "   ...  thing  Good  late-night  zoned-out  viewing  only  You  have  \\\n",
      "0  ...      1     1           1          1        1     1    1     1   \n",
      "1  ...      0     0           0          0        0     0    0     2   \n",
      "2  ...      1     0           0          0        0     1    0     1   \n",
      "3  ...      0     0           0          0        0     0    0     0   \n",
      "4  ...      0     0           0          0        0     2    0     0   \n",
      "\n",
      "   been  warned  \n",
      "0     1       1  \n",
      "1     0       0  \n",
      "2     0       0  \n",
      "3     0       0  \n",
      "4     0       0  \n",
      "\n",
      "[5 rows x 108 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import casual_tokenize\n",
    "from collections import Counter\n",
    "pd.set_option('display.width', 75)\n",
    "bags_of_words = []\n",
    "\n",
    "# tokenize each row, append into a list of dicts and convert to dataframe\n",
    "for text in movies.review.to_list():\n",
    "    bags_of_words.append(Counter(casual_tokenize(text)))\n",
    "df_bows = pd.DataFrame.from_records(bags_of_words)\n",
    "df_bows = df_bows.fillna(0).astype(int)\n",
    "\n",
    "print('bows shape:', df_bows.shape)\n",
    "print('**************************************************')\n",
    "print('first review:', movies.loc[0].review)\n",
    "print('**************************************************')\n",
    "print('tokens of the first review that appear in the first 5 rows of the dataset', \n",
    "      df_bows.head()[list(bags_of_words[0].keys())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict the sentiment and computing metrics for accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 2.0512326689917173\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.993"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "\n",
    "# fit the model and convert continuous to boolean label\n",
    "nb = nb.fit(df_bows, movies.sentiment > 0)\n",
    "\n",
    "# values from review columns goes from -4 to 4 so this code normalize to the \"ground true\" sentiment\n",
    "movies['predicted_sentiment'] = nb.predict_proba(df_bows)[:,1] * 8 - 4  \n",
    "movies['error'] = (movies.predicted_sentiment - movies.sentiment).abs()\n",
    "\n",
    "# metrics\n",
    "print('MAE:', movies.error.mean()) \n",
    "\n",
    "# support columns\n",
    "movies['sentiment_ispositive'] = (movies.sentiment > 0).astype(int)\n",
    "movies['predicted_ispositive'] = (movies.predicted_sentiment > 0).astype(int)\n",
    "movies['''sentiment predicted_sentiment sentiment_ispositive predicted_ispositive'''.split()].head(8)\n",
    "\n",
    "#     sentiment  predicted_sentiment  sentiment_ispositive  predicted_ispositive\n",
    "# id\n",
    "# 1    2.266667                   4                    1                    1\n",
    "# 2    3.533333                   4                    1                    1\n",
    "# 3   -0.600000                  -4                    0                    0\n",
    "# 4    1.466667                   4                    1                    1\n",
    "# 5    1.733333                   4                    1                    1\n",
    "# 6    2.533333                   4                    1                    1\n",
    "# 7    2.466667                   4                    1                    1\n",
    "# 8    1.266667                  -4                    1                    0\n",
    "\n",
    "# prediction over positives\n",
    "(movies.predicted_ispositive == movies.sentiment_ispositive).sum() / len(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.predict_proba( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
