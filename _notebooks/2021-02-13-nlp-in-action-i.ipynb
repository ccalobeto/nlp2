{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP in Action I\n",
    "> My journey to learn NLP\n",
    "\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: false\n",
    "- comments: false\n",
    "- categories: [fastpages, jupyter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install a pip package in the current Jupyter kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# !{sys.executable} -m pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### basics of regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [] indicates character class\n",
    "- r'[\\s]' is equivalent to r'\\t\\n\\r\\x0b\\x0c' match all the spaces, tabs, returns, new lines and \n",
    " form-feed\n",
    "- r'[a-z]' match all lowercase\n",
    "- r'[0-9]' match any digit\n",
    "- r'[_a-zA-Z]' match any underscore character or letter of the english alphabet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Match any sentence that begins with hi|hello|hey followed by space(s) and a word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = \"(hi|hello|hey)[ ]*([a-z]*)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 10), match='Hello Rosa'>\n",
      "None\n",
      "<re.Match object; span=(0, 9), match='hey, what'>\n"
     ]
    }
   ],
   "source": [
    "print(re.match(r, 'Hello Rosa', flags=re.IGNORECASE))\n",
    "print(re.match(r, \"hi ho, hi ho, it's off to work ...\", flags=re.IGNORECASE))\n",
    "print(re.match(r, \"hey, what's up\", flags=re.IGNORECASE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example with a complex pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = r\"[^a-z]*([y]o|[h']?ello|ok|hey|(good[ ])?(morn[gin']{0,3}|\"\\\n",
    "     r\"afternoon|even[gin']{0,3}))[\\s,;:]{1,3}([a-z]{1,20})\"\n",
    "\n",
    "re_greeting = re.compile(r, flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 10), match='Hello Rosa'>\n",
      "('Hello', None, None, 'Rosa')\n",
      "<re.Match object; span=(0, 17), match='Good morning Rosa'>\n",
      "None\n",
      "('Good evening', 'Good ', 'evening', 'Rosa')\n",
      "<re.Match object; span=(0, 16), match=\"Good Morn'n Rosa\">\n",
      "<re.Match object; span=(0, 7), match='yo Rosa'>\n"
     ]
    }
   ],
   "source": [
    "print(re_greeting.match('Hello Rosa'))\n",
    "print(re_greeting.match('Hello Rosa').groups())\n",
    "print(re_greeting.match(\"Good morning Rosa\"))\n",
    "print(re_greeting.match(\"Good Manning Rosa\"))\n",
    "print(re_greeting.match('Good evening Rosa Parks').groups())\n",
    "print(re_greeting.match(\"Good Morn'n Rosa\"))\n",
    "print(re_greeting.match(\"yo Rosa\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Chat Bot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter the text 'good morning rosa or hello rose'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " hello rose\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi , How are you?\n"
     ]
    }
   ],
   "source": [
    "my_names = set(['rosa', 'rose', 'chatty', 'chatbot', 'bot', 'chatterbot'])\n",
    "curt_names = set(['hal', 'you', 'u'])\n",
    "greeter_name = ''\n",
    "match = re_greeting.match(input())\n",
    "\n",
    "if match:\n",
    "    at_name = match.groups()[-1]\n",
    "    if at_name in curt_names:\n",
    "        print(\"Good one.\")\n",
    "\n",
    "    elif at_name.lower() in my_names:\n",
    "        print(\"Hi {}, How are you?\".format(greeter_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Permutations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n=3 permutations with text 'Good morning Rosa!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good morning Rosa!', 'Good Rosa! morning', 'morning Good Rosa!', 'morning Rosa! Good', 'Rosa! Good morning', 'Rosa! morning Good']\n"
     ]
    }
   ],
   "source": [
    "from itertools import permutations\n",
    "print([\" \".join(combo) for combo in permutations(\"Good morning Rosa!\".split(), 3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting words using Counter. Dict output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Guten': 1, 'Morgen': 1, 'Rosa': 1})\n",
      "Counter({'morning': 2, 'Good': 1, ',': 1, 'Rosa!': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(Counter(\"Guten Morgen Rosa\".split()))\n",
    "print(Counter(\"Good morning morning , Rosa!\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thomas',\n",
       " 'Jefferson',\n",
       " 'began',\n",
       " 'building',\n",
       " 'Monticello',\n",
       " 'at',\n",
       " 'age',\n",
       " 'of',\n",
       " '26.']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Thomas Jefferson began building Monticello at age of 26.\"\n",
    "sentence.split()\n",
    "str.split(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split a sentence into tokens, order it and convert to a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26., Jefferson, Monticello, Thomas, age, at, began, building, of\n",
      "*********************\n",
      "[[0 0 0 1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 1 0]\n",
      " [0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1]\n",
      " [1 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "token_sequence = str.split(sentence)\n",
    "vocab = sorted(set(token_sequence))\n",
    "num_tokens = len(token_sequence)\n",
    "vocab_size = len(vocab)\n",
    "onehot_vectors = np.zeros((num_tokens, vocab_size), int)\n",
    "\n",
    "for i, word in enumerate(token_sequence):\n",
    "    onehot_vectors[i, vocab.index(word)] = 1\n",
    "\n",
    "print(', '.join(vocab))\n",
    "print('*********************')\n",
    "print(onehot_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a dataframe with matrix of vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   26.  Jefferson  Monticello  Thomas  age  at  began  building  of\n",
      "0    0          0           0       1    0   0      0         0   0\n",
      "1    0          1           0       0    0   0      0         0   0\n",
      "2    0          0           0       0    0   0      1         0   0\n",
      "3    0          0           0       0    0   0      0         1   0\n",
      "4    0          0           1       0    0   0      0         0   0\n",
      "5    0          0           0       0    0   1      0         0   0\n",
      "6    0          0           0       0    1   0      0         0   0\n",
      "7    0          0           0       0    0   0      0         0   1\n",
      "8    1          0           0       0    0   0      0         0   0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(pd.DataFrame(onehot_vectors, columns=vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "construct a frequency vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text to analize: Thomas Jefferson began building Monticello at the age of 26.\n",
      "Construction was done mostly by local masons and carpenters.\n",
      "He moved into the South Pavilion in 1770.\n",
      "Turning Monticello into a neoclassical masterpiece was Jefferson's obsession.\n",
      "*********************************************************\n",
      "first dict: {'Thomas': 1, 'Jefferson': 1, 'began': 1, 'building': 1, 'Monticello': 1, 'at': 1, 'the': 1, 'age': 1, 'of': 1, '26.': 1}\n",
      "*********************************************************\n",
      "       Thomas  Jefferson  began  building  Monticello  at  the  age  of  26.\n",
      "sent0       1          1      1         1           1   1    1    1   1    1\n",
      "sent1       0          0      0         0           0   0    0    0   0    0\n",
      "sent2       0          0      0         0           0   0    1    0   0    0\n",
      "sent3       0          0      0         0           1   0    0    0   0    0\n",
      "**********************************************************\n",
      "word shared by two sentences: [('Monticello', 1)]\n"
     ]
    }
   ],
   "source": [
    "sentences = \"\"\"Thomas Jefferson began building Monticello at the age of 26.\\n\"\"\"\n",
    "sentences += \"\"\"Construction was done mostly by local masons and carpenters.\\n\"\"\"\n",
    "sentences += \"He moved into the South Pavilion in 1770.\\n\"\n",
    "sentences += \"\"\"Turning Monticello into a neoclassical masterpiece was Jefferson's obsession.\"\"\"\n",
    "\n",
    "print('text to analize:', sentences)\n",
    "print('*********************************************************')\n",
    "\n",
    "#1. construct a dict of dicts\n",
    "corpus = {}\n",
    "for i, sent in enumerate(sentences.split('\\n')):\n",
    "    corpus['sent{}'.format(i)] = dict((tok, 1) for tok in sent.split()) \n",
    "\n",
    "print('first dict:', corpus['sent0'])\n",
    "print('*********************************************************')\n",
    "#2. convert dict to dataframe \n",
    "df = pd.DataFrame.from_records(corpus).fillna(0).astype(int).T\n",
    "print(df[df.columns[:10]])\n",
    "#\t    Thomas\tJefferson\tbegan\tbuilding\tMonticello\tat\tthe\tage\tof\t26.\n",
    "# sent0\t    1\t    1\t    1\t        1\t        1\t    1\t1\t1\t1\t1\n",
    "# sent1\t    0\t    0\t    0\t        0\t        0\t    0\t0\t0\t0\t0\n",
    "# sent2\t    0\t    0\t    0\t        0\t        0\t    0\t1\t0\t0\t0\n",
    "# sent3\t    0\t    0\t    0\t        0\t        1\t    0\t0\t0\t0\t0\n",
    "\n",
    "print('**********************************************************')\n",
    "print('word shared by two sentences:',[(k, v) for (k, v) in (df.loc['sent0'] & df.loc['sent3']).\n",
    "                                       items() if v])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split into tokens using [regular expressions](#### basics of regular expressions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list of tokens: ['Thomas', 'Jefferson', 'began', 'building', 'Monticello', 'at', 'the', 'age', 'of', '26', '']\n",
      "***********************************************\n",
      "last 10 tokens: [' ', 'the', ' ', 'age', ' ', 'of', ' ', '26', '.', '']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "sentence = \"\"\"Thomas Jefferson began building Monticello at the age of 26.\"\"\"\n",
    "tokens = re.split(r'[-\\s.,;!?]+', sentence)\n",
    "print('list of tokens:', tokens)\n",
    "print('***********************************************')\n",
    "# a better version of tokenization fast and manageable\n",
    "pattern = re.compile(r\"([-\\s.,;!?])+\")\n",
    "tokens = pattern.split(sentence)\n",
    "print('last 10 tokens:', tokens[-10:])  \n",
    "# ['the', ' ', 'age', ' ', 'of', ' ', '26', '.', '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ignoring punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing punctuation: ['Thomas', 'Jefferson', 'began', 'building', 'Monticello', 'at', 'the', 'age', 'of', '26']\n"
     ]
    }
   ],
   "source": [
    "print('removing punctuation:', [x for x in tokens if x not in '- \\t\\n.,;!?'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ignoring whitespaces with tokeneizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Thomas', 'Jefferson', 'began', 'building', 'Monticello', 'at', 'the', 'age', 'of', '26', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+|$[0-9.]+|\\S+')\n",
    "print(tokenizer.tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### managing contractions with tokeneizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Monticello', 'was', \"n't\", 'designated', 'as', 'UNESCO', 'World', 'Heritage', 'Site', 'until', '1987', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "sentence = \"\"\"Monticello wasn't designated as UNESCO World Heritage Site until 1987.\"\"\"\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "print(tokenizer.tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tokenize informal conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: ['RT', '@TJMonticello', 'Best', 'day', 'everrrrrrr', 'at', 'Monticello', '.', 'Awesommmmmmeeeeeeee', 'day', ':*)']\n",
      "***************************************************\n",
      "best approach to tokens: ['RT', 'Best', 'day', 'everrr', 'at', 'Monticello', '.', 'Awesommmeee', 'day', ':*)']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize.casual import casual_tokenize\n",
    "message = \"\"\"RT @TJMonticello Best day everrrrrrr at Monticello. Awesommmmmmeeeeeeee day :*)\"\"\"\n",
    "print('tokens:', casual_tokenize(message))\n",
    "\n",
    "print('***************************************************')\n",
    "print('best approach to tokens:', casual_tokenize(message, reduce_len=True, strip_handles=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list of tuples: [('Thomas', 'Jefferson'), ('Jefferson', 'began'), ('began', 'building'), ('building', 'Monticello'), ('Monticello', 'at'), ('at', 'the'), ('the', 'age'), ('age', 'of'), ('of', '26')]\n",
      "*****************************************************\n",
      "list of triplets: [('Thomas', 'Jefferson', 'began'), ('Jefferson', 'began', 'building'), ('began', 'building', 'Monticello'), ('building', 'Monticello', 'at'), ('Monticello', 'at', 'the'), ('at', 'the', 'age'), ('the', 'age', 'of'), ('age', 'of', '26')]\n",
      "*****************************************************\n",
      "list of 2-grams ['Thomas Jefferson', 'Jefferson began', 'began building', 'building Monticello', 'Monticello at', 'at the', 'the age', 'age of', 'of 26']\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "print('list of tuples:', list(ngrams(tokens, 2)))\n",
    "print('*****************************************************')\n",
    "print('list of triplets:', list(ngrams(tokens, 3)))\n",
    "print('*****************************************************')\n",
    "print('list of 2-grams', [' '.join(x) for x in list(ngrams(tokens, 2))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens without stopwords: ['house', 'fire']\n",
      "first seven words ['i', 'me', 'my', 'myself', 'we', 'our', 'ours']\n",
      "stopwords with 1 character in nltk: ['i', 'a', 's', 't', 'd', 'm', 'o', 'y']\n",
      "nltk stopwords: 179\n",
      "sklearn stopwords: 318\n"
     ]
    }
   ],
   "source": [
    "# removing stopwords be careful, depends on the application\n",
    "stop_words = ['a', 'an', 'the', 'on', 'of', 'off', 'this', 'is']\n",
    "tokens = ['the', 'house', 'is', 'on', 'fire']\n",
    "tokens_without_stopwords = [x for x in tokens if x not in stop_words]\n",
    "print('tokens without stopwords:', tokens_without_stopwords)\n",
    "\n",
    "# canonical stopwords\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "# print('size of nltk stopwords:', len(stop_words))\n",
    "print('first seven words', stop_words[:7])\n",
    "# ['i', 'me', 'my', 'myself', 'we', 'our', 'ours']\n",
    "print('stopwords with 1 character in nltk:', [sw for sw in stop_words if len(sw) == 1])\n",
    "# ['i', 'a', 's', 't', 'd', 'm', 'o', 'y']\n",
    "\n",
    "# stopwords comparison between sklearn and NLTK\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as sklearn_stop_words\n",
    "nltk_stop_words = nltk.corpus.stopwords.words('english')\n",
    "print('nltk stopwords:', len(nltk_stop_words))\n",
    "print('sklearn stopwords:', len(sklearn_stop_words))\n",
    "\n",
    "#print(len(nltk_stop_words.union(sklearn_stop_words)))\n",
    "#print(len(nltk_stop_words.intersection(sklearn_stop_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing capitalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['house', 'visitor', 'center']\n"
     ]
    }
   ],
   "source": [
    "tokens = ['House', 'Visitor', 'Center']\n",
    "print([x.lower() for x in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### stemmer with regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "house\n",
      "doctor house call\n"
     ]
    }
   ],
   "source": [
    "def stem(phrase):\n",
    "    return ' '.join([re.findall('^(.*ss|.*?)(s)?$', word)[0][0].strip(\"'\") for word in phrase.lower().split()])\n",
    "\n",
    "print(stem('houses'))\n",
    "print(stem(\"Doctor House's calls\"))\n",
    "# doctor house call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### complete stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dish washer wash dish'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "' '.join([stemmer.stem(w).strip(\"'\") for w in \"dish washer's wash dishes\".split()])\n",
    "# dish washer wash dish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "better\n",
      "good\n",
      "good\n",
      "goods\n",
      "good\n",
      "goodness\n",
      "best\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# if post isn't specified the lemmatizer assumes it is a noun\n",
    "print(lemmatizer.lemmatize('better'))\n",
    "print(lemmatizer.lemmatize('better', pos='a'))\n",
    "print(lemmatizer.lemmatize('good', pos='a'))\n",
    "print(lemmatizer.lemmatize('goods', pos='a'))\n",
    "print(lemmatizer.lemmatize('goods', pos='n'))\n",
    "print(lemmatizer.lemmatize('goodness', pos='n'))\n",
    "print(lemmatizer.lemmatize('best', pos='a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vader model for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lexicon words with spaces [(\"( '}{' )\", 1.6), (\"can't stand\", -2.0), ('fed up', -1.8), ('screwed up', -1.5)]\n",
      "*******************************************\n",
      "dict sentiment for text1: {'neg': 0.0, 'neu': 0.687, 'pos': 0.313, 'compound': 0.6249}\n",
      "dict sentiment for text2: {'neg': 0.0, 'neu': 0.737, 'pos': 0.263, 'compound': 0.431}\n"
     ]
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sa = SentimentIntensityAnalyzer()\n",
    "text1 = \"Python is very readable and it is great for NLP.\"\n",
    "text2 = \"Python is not a bad choice for most applications.\"\n",
    "\n",
    "print('lexicon words with spaces', [(tok, score) for tok, score in sa.lexicon.items() if \" \" in tok])\n",
    "print('*******************************************')\n",
    "print('dict sentiment for text1:', sa.polarity_scores(text=text1))\n",
    "print('dict sentiment for text2:', sa.polarity_scores(text=text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sentiment score for a given corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+0.9428: Absolutely perfect! Love it! :-) :-) :-)\n",
      "-0.8768: Horrible! Completely useless. :(\n",
      "-0.1531: It was Ok. Some good and some bad things\n"
     ]
    }
   ],
   "source": [
    "corpus = [\"Absolutely perfect! Love it! :-) :-) :-)\",\n",
    "          \"Horrible! Completely useless. :(\",\n",
    "          \"It was Ok. Some good and some bad things\"]\n",
    "\n",
    "for doc in corpus:\n",
    "    scores = sa.polarity_scores(doc)\n",
    "    print('{:+}: {}'.format(scores['compound'], doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### naive model for sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code to use movies data with nlpia but i can't install it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive bayes model\n",
    "#from nlpia.data.loaders import get_data\n",
    "#movies = get_data('hutto_movies')\n",
    "#movies.head().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate sentiment from movie data. Remember you can't install nlpia where the data movie is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Woman (Miriam Hopkins as Virginia) chases Man ...</td>\n",
       "      <td>3.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The combination of reading the Novella and vie...</td>\n",
       "      <td>-3.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When my now college age daughter was in presch...</td>\n",
       "      <td>2.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Absolutely awful movie. Utter waste of time.&lt;b...</td>\n",
       "      <td>-0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I disagree with previous comment about this mo...</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>It's rare that I feel a need to write a review...</td>\n",
       "      <td>-0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Regardless of what personal opinion one may ha...</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>I have recently seen this movie due to Jake's ...</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>There is absolutely no plot in this movie ...n...</td>\n",
       "      <td>-2.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>After gorging myself on a variety of seemingly...</td>\n",
       "      <td>-1.87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                review  sentiment\n",
       "0    Woman (Miriam Hopkins as Virginia) chases Man ...       3.49\n",
       "1    The combination of reading the Novella and vie...      -3.81\n",
       "2    When my now college age daughter was in presch...       2.21\n",
       "3    Absolutely awful movie. Utter waste of time.<b...      -0.94\n",
       "4    I disagree with previous comment about this mo...       0.14\n",
       "..                                                 ...        ...\n",
       "995  It's rare that I feel a need to write a review...      -0.86\n",
       "996  Regardless of what personal opinion one may ha...       0.90\n",
       "997  I have recently seen this movie due to Jake's ...       0.18\n",
       "998  There is absolutely no plot in this movie ...n...      -2.02\n",
       "999  After gorging myself on a variety of seemingly...      -1.87\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "reviews_train = []\n",
    "for line in open('movie_data/full_train.txt', 'r'):\n",
    "    reviews_train.append(line.strip())\n",
    "movies = pd.DataFrame(reviews_train, columns=['review'])\n",
    "\n",
    "movies['sentiment'] = np.random.uniform(-4, 4, 25000).round(2)\n",
    "# sample the data\n",
    "movies = movies.sample(n=1000).reset_index(drop=True)\n",
    "movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentiment goes from -4 to 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.015730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.362015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-3.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-2.055000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.145000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment\n",
       "count  1000.000000\n",
       "mean     -0.015730\n",
       "std       2.362015\n",
       "min      -3.990000\n",
       "25%      -2.055000\n",
       "50%      -0.145000\n",
       "75%       2.180000\n",
       "max       4.000000"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert 'review' column to bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bows shape: (1000, 22298)\n",
      "**************************************************\n",
      "first review:\n",
      "I'll give writer/director William Gove credit for finding someone to finance this ill-conceived \"thriller.\" A good argument for not wasting money subscribing to HBO, let alone buying DVDs based on cover art and blurbs. A pedestrian Dennis Hopper and a game Richard Grieco add nothing significant to their resumes, although the art direction is not half bad. The dialogue will leave you grimacing with wonder at its conceit; this is storytelling at its worst. No tension, no suspense, no dread, no fear, no empathy, no catharsis, no nothing. A few attractive and often nude females spice up the boredom, but this is definitely a film best seen as a trailer. I feel sorry for the guy who greenlighted this thing. Good for late-night, zoned-out viewing only. You have been warned.\n",
      "**************************************************\n",
      "108 tokens of the first review, with a lexicon of 22298 tokens\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I'll</th>\n",
       "      <th>give</th>\n",
       "      <th>writer</th>\n",
       "      <th>/</th>\n",
       "      <th>director</th>\n",
       "      <th>William</th>\n",
       "      <th>Gove</th>\n",
       "      <th>credit</th>\n",
       "      <th>for</th>\n",
       "      <th>finding</th>\n",
       "      <th>...</th>\n",
       "      <th>thing</th>\n",
       "      <th>Good</th>\n",
       "      <th>late-night</th>\n",
       "      <th>zoned-out</th>\n",
       "      <th>viewing</th>\n",
       "      <th>only</th>\n",
       "      <th>You</th>\n",
       "      <th>have</th>\n",
       "      <th>been</th>\n",
       "      <th>warned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 108 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   I'll  give  writer   /  director  William  Gove  credit  for  finding  \\\n",
       "0     1     1       1   1         1        1     1       1    4        1   \n",
       "1     0     0       0  12         0        0     0       0    2        0   \n",
       "2     0     0       0   0         0        0     0       0    1        0   \n",
       "3     0     0       0   6         1        0     0       0    3        0   \n",
       "4     0     0       0   4         0        0     0       0    2        0   \n",
       "\n",
       "   ...  thing  Good  late-night  zoned-out  viewing  only  You  have  \\\n",
       "0  ...      1     1           1          1        1     1    1     1   \n",
       "1  ...      0     0           0          0        0     0    0     2   \n",
       "2  ...      1     0           0          0        0     1    0     1   \n",
       "3  ...      0     0           0          0        0     0    0     0   \n",
       "4  ...      0     0           0          0        0     2    0     0   \n",
       "\n",
       "   been  warned  \n",
       "0     1       1  \n",
       "1     0       0  \n",
       "2     0       0  \n",
       "3     0       0  \n",
       "4     0       0  \n",
       "\n",
       "[5 rows x 108 columns]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import casual_tokenize\n",
    "from collections import Counter\n",
    "pd.set_option('display.width', 75)\n",
    "bags_of_words = []\n",
    "\n",
    "# tokenize each row, append into a list of dicts and convert to dataframe\n",
    "for text in movies.review.to_list():\n",
    "    bags_of_words.append(Counter(casual_tokenize(text)))\n",
    "df_bows = pd.DataFrame.from_records(bags_of_words)\n",
    "df_bows = df_bows.fillna(0).astype(int)\n",
    "\n",
    "print('bows shape:', df_bows.shape)\n",
    "print('**************************************************')\n",
    "print('first review:') \n",
    "print(movies.loc[0].review)\n",
    "print('**************************************************')\n",
    "print('108 tokens of the first review, with a lexicon of 22298 tokens') \n",
    "df_bows.head()[list(bags_of_words[0].keys())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict the sentiment and computing metrics for accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 2.0512326689917173\n",
      "positives accuracy: 0.993\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "\n",
    "# fit the model and convert continuous to boolean label\n",
    "nb = nb.fit(df_bows, movies.sentiment > 0)\n",
    "\n",
    "# values from review columns goes from -4 to 4 so this code normalize to the \"ground true\" \n",
    "# sentiment. predict_proba return a n x 2 ndarray, we use el \"1s\" column\n",
    "movies['predicted_sentiment'] = nb.predict_proba(df_bows)[:,1] * 8 - 4  \n",
    "movies['error'] = (movies.predicted_sentiment - movies.sentiment).abs()\n",
    "\n",
    "# metrics\n",
    "print('MAE:', movies.error.mean()) \n",
    "\n",
    "# support columns\n",
    "movies['sentiment_ispositive'] = (movies.sentiment > 0).astype(int)\n",
    "movies['predicted_ispositive'] = (movies.predicted_sentiment > 0).astype(int)\n",
    "movies['''sentiment predicted_sentiment sentiment_ispositive predicted_ispositive'''.split()].head(8)\n",
    "\n",
    "# prediction over positives\n",
    "print('positives accuracy:', (movies.predicted_ispositive == \n",
    "                             movies.sentiment_ispositive).sum() / len(movies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Math with words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>the</th>\n",
       "      <th>faster</th>\n",
       "      <th>harry</th>\n",
       "      <th>got</th>\n",
       "      <th>to</th>\n",
       "      <th>store</th>\n",
       "      <th>,</th>\n",
       "      <th>would</th>\n",
       "      <th>get</th>\n",
       "      <th>home</th>\n",
       "      <th>.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.36</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    the  faster  harry   got    to  store     ,  would   get  home     .\n",
       "0  0.36    0.27   0.18  0.09  0.09   0.09  0.27   0.09  0.09  0.09  0.09"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from collections import Counter\n",
    "sentence = \"\"\"The faster Harry got to the store, the faster Harry, the faster, would get home.\"\"\"\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(sentence.lower())\n",
    "bags_of_words = Counter(tokens)\n",
    "bags_of_words.most_common(4)  # top 4 bag of words\n",
    "bags_of_words_df = pd.DataFrame.from_records([bags_of_words])  # put the tokens on columns, give the form of many sentences\n",
    "tf = (bags_of_words_df / len(bags_of_words)).round(2)\n",
    "tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag Of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "# van't install nlpia but from nlpia.data.loaders import kite_text\n",
    "\n",
    "kite_text = \"\"\"A kite is traditionally a tethered heavier-than-air craft with wing surfaces that react against the air \n",
    "to create lift and drag. A kite consists of wings, tethers, and anchors. Kites often have a bridle to guide the face of \n",
    "the kite at the correct angle so the wind can lift it. A kite’s wing also may be so designed so a bridle is not needed;\n",
    "when kiting a sailplane for launch, the tether meets the wing at a single point. A kite may have fixed or moving anchors. \n",
    "Untraditionally in technical kiting, a kite consists of tether-set-coupled wing sets; even in technical kiting, though, \n",
    "a wing in the system is still often called the kite.\n",
    "The lift that sustains the kite in flight is generated when air flows around the kite’s surface, producing low pressure \n",
    "above and high pressure below the wings. The interaction with the wind also generates horizontal drag along the direction \n",
    "of the wind. The resultant force vector from the lift and drag force components is opposed by the tension of one or more \n",
    "of the lines or tethers to which the kite is attached. The anchor point of the kite line may be static or moving (such \n",
    "as the towing of a kite by a running person, boat, free-falling anchors as in paragliders and fugitive parakites or \n",
    "vehicle).\n",
    "The same principles of fluid flow apply in liquids and kites are also used under water.\n",
    "A hybrid tethered craft comprising both a lighter-than-air balloon as well as a kite lifting surface is called a kytoon.\n",
    "Kites have a long and varied history and many different types are flown individually and at festivals worldwide. Kites \n",
    "may be flown for recreation, art or other practical uses. Sport kites can be flown in aerial ballet, sometimes as part \n",
    "of a competition. Power kites are multi-line steerable kites designed to generate large forces which can be used to \n",
    "power activities such as kite surfing, kite landboarding, kite fishing, kite buggying and a new trend snow kiting. \n",
    "Even Man-lifting kites have been made.\n",
    "\"\"\"\n",
    "tokens = tokenizer.tokenize(kite_text.lower())\n",
    "token_counts = Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'a': 20,\n",
       "         'kite': 14,\n",
       "         'is': 7,\n",
       "         'traditionally': 1,\n",
       "         'tethered': 2,\n",
       "         'heavier-than-air': 1,\n",
       "         'craft': 2,\n",
       "         'with': 2,\n",
       "         'wing': 5,\n",
       "         'surfaces': 1,\n",
       "         'that': 2,\n",
       "         'react': 1,\n",
       "         'against': 1,\n",
       "         'the': 26,\n",
       "         'air': 2,\n",
       "         'to': 5,\n",
       "         'create': 1,\n",
       "         'lift': 4,\n",
       "         'and': 10,\n",
       "         'drag.': 1,\n",
       "         'consists': 2,\n",
       "         'of': 10,\n",
       "         'wings': 1,\n",
       "         ',': 14,\n",
       "         'tethers': 2,\n",
       "         'anchors.': 2,\n",
       "         'kites': 8,\n",
       "         'often': 2,\n",
       "         'have': 4,\n",
       "         'bridle': 2,\n",
       "         'guide': 1,\n",
       "         'face': 1,\n",
       "         'at': 3,\n",
       "         'correct': 1,\n",
       "         'angle': 1,\n",
       "         'so': 3,\n",
       "         'wind': 2,\n",
       "         'can': 3,\n",
       "         'it.': 1,\n",
       "         'kite’s': 2,\n",
       "         'also': 3,\n",
       "         'may': 4,\n",
       "         'be': 5,\n",
       "         'designed': 2,\n",
       "         'not': 1,\n",
       "         'needed': 1,\n",
       "         ';': 2,\n",
       "         'when': 2,\n",
       "         'kiting': 3,\n",
       "         'sailplane': 1,\n",
       "         'for': 2,\n",
       "         'launch': 1,\n",
       "         'tether': 1,\n",
       "         'meets': 1,\n",
       "         'single': 1,\n",
       "         'point.': 1,\n",
       "         'fixed': 1,\n",
       "         'or': 6,\n",
       "         'moving': 2,\n",
       "         'untraditionally': 1,\n",
       "         'in': 7,\n",
       "         'technical': 2,\n",
       "         'tether-set-coupled': 1,\n",
       "         'sets': 1,\n",
       "         'even': 2,\n",
       "         'though': 1,\n",
       "         'system': 1,\n",
       "         'still': 1,\n",
       "         'called': 2,\n",
       "         'kite.': 1,\n",
       "         'sustains': 1,\n",
       "         'flight': 1,\n",
       "         'generated': 1,\n",
       "         'flows': 1,\n",
       "         'around': 1,\n",
       "         'surface': 2,\n",
       "         'producing': 1,\n",
       "         'low': 1,\n",
       "         'pressure': 2,\n",
       "         'above': 1,\n",
       "         'high': 1,\n",
       "         'below': 1,\n",
       "         'wings.': 1,\n",
       "         'interaction': 1,\n",
       "         'generates': 1,\n",
       "         'horizontal': 1,\n",
       "         'drag': 2,\n",
       "         'along': 1,\n",
       "         'direction': 1,\n",
       "         'wind.': 1,\n",
       "         'resultant': 1,\n",
       "         'force': 2,\n",
       "         'vector': 1,\n",
       "         'from': 1,\n",
       "         'components': 1,\n",
       "         'opposed': 1,\n",
       "         'by': 2,\n",
       "         'tension': 1,\n",
       "         'one': 1,\n",
       "         'more': 1,\n",
       "         'lines': 1,\n",
       "         'which': 2,\n",
       "         'attached.': 1,\n",
       "         'anchor': 1,\n",
       "         'point': 1,\n",
       "         'line': 1,\n",
       "         'static': 1,\n",
       "         '(': 1,\n",
       "         'such': 2,\n",
       "         'as': 6,\n",
       "         'towing': 1,\n",
       "         'running': 1,\n",
       "         'person': 1,\n",
       "         'boat': 1,\n",
       "         'free-falling': 1,\n",
       "         'anchors': 1,\n",
       "         'paragliders': 1,\n",
       "         'fugitive': 1,\n",
       "         'parakites': 1,\n",
       "         'vehicle': 1,\n",
       "         ')': 1,\n",
       "         '.': 2,\n",
       "         'same': 1,\n",
       "         'principles': 1,\n",
       "         'fluid': 1,\n",
       "         'flow': 1,\n",
       "         'apply': 1,\n",
       "         'liquids': 1,\n",
       "         'are': 3,\n",
       "         'used': 2,\n",
       "         'under': 1,\n",
       "         'water.': 1,\n",
       "         'hybrid': 1,\n",
       "         'comprising': 1,\n",
       "         'both': 1,\n",
       "         'lighter-than-air': 1,\n",
       "         'balloon': 1,\n",
       "         'well': 1,\n",
       "         'lifting': 1,\n",
       "         'kytoon.': 1,\n",
       "         'long': 1,\n",
       "         'varied': 1,\n",
       "         'history': 1,\n",
       "         'many': 1,\n",
       "         'different': 1,\n",
       "         'types': 1,\n",
       "         'flown': 3,\n",
       "         'individually': 1,\n",
       "         'festivals': 1,\n",
       "         'worldwide.': 1,\n",
       "         'recreation': 1,\n",
       "         'art': 1,\n",
       "         'other': 1,\n",
       "         'practical': 1,\n",
       "         'uses.': 1,\n",
       "         'sport': 1,\n",
       "         'aerial': 1,\n",
       "         'ballet': 1,\n",
       "         'sometimes': 1,\n",
       "         'part': 1,\n",
       "         'competition.': 1,\n",
       "         'power': 2,\n",
       "         'multi-line': 1,\n",
       "         'steerable': 1,\n",
       "         'generate': 1,\n",
       "         'large': 1,\n",
       "         'forces': 1,\n",
       "         'activities': 1,\n",
       "         'surfing': 1,\n",
       "         'landboarding': 1,\n",
       "         'fishing': 1,\n",
       "         'buggying': 1,\n",
       "         'new': 1,\n",
       "         'trend': 1,\n",
       "         'snow': 1,\n",
       "         'kiting.': 1,\n",
       "         'man-lifting': 1,\n",
       "         'been': 1,\n",
       "         'made': 1})"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse\n",
    "token_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('stopwords', quiet=True)\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "tokens = [x for x in tokens if x not in stopwords]\n",
    "kite_counts = Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'kite': 14,\n",
       "         'traditionally': 1,\n",
       "         'tethered': 2,\n",
       "         'heavier-than-air': 1,\n",
       "         'craft': 2,\n",
       "         'wing': 5,\n",
       "         'surfaces': 1,\n",
       "         'react': 1,\n",
       "         'air': 2,\n",
       "         'create': 1,\n",
       "         'lift': 4,\n",
       "         'drag.': 1,\n",
       "         'consists': 2,\n",
       "         'wings': 1,\n",
       "         ',': 14,\n",
       "         'tethers': 2,\n",
       "         'anchors.': 2,\n",
       "         'kites': 8,\n",
       "         'often': 2,\n",
       "         'bridle': 2,\n",
       "         'guide': 1,\n",
       "         'face': 1,\n",
       "         'correct': 1,\n",
       "         'angle': 1,\n",
       "         'wind': 2,\n",
       "         'it.': 1,\n",
       "         'kite’s': 2,\n",
       "         'also': 3,\n",
       "         'may': 4,\n",
       "         'designed': 2,\n",
       "         'needed': 1,\n",
       "         ';': 2,\n",
       "         'kiting': 3,\n",
       "         'sailplane': 1,\n",
       "         'launch': 1,\n",
       "         'tether': 1,\n",
       "         'meets': 1,\n",
       "         'single': 1,\n",
       "         'point.': 1,\n",
       "         'fixed': 1,\n",
       "         'moving': 2,\n",
       "         'untraditionally': 1,\n",
       "         'technical': 2,\n",
       "         'tether-set-coupled': 1,\n",
       "         'sets': 1,\n",
       "         'even': 2,\n",
       "         'though': 1,\n",
       "         'system': 1,\n",
       "         'still': 1,\n",
       "         'called': 2,\n",
       "         'kite.': 1,\n",
       "         'sustains': 1,\n",
       "         'flight': 1,\n",
       "         'generated': 1,\n",
       "         'flows': 1,\n",
       "         'around': 1,\n",
       "         'surface': 2,\n",
       "         'producing': 1,\n",
       "         'low': 1,\n",
       "         'pressure': 2,\n",
       "         'high': 1,\n",
       "         'wings.': 1,\n",
       "         'interaction': 1,\n",
       "         'generates': 1,\n",
       "         'horizontal': 1,\n",
       "         'drag': 2,\n",
       "         'along': 1,\n",
       "         'direction': 1,\n",
       "         'wind.': 1,\n",
       "         'resultant': 1,\n",
       "         'force': 2,\n",
       "         'vector': 1,\n",
       "         'components': 1,\n",
       "         'opposed': 1,\n",
       "         'tension': 1,\n",
       "         'one': 1,\n",
       "         'lines': 1,\n",
       "         'attached.': 1,\n",
       "         'anchor': 1,\n",
       "         'point': 1,\n",
       "         'line': 1,\n",
       "         'static': 1,\n",
       "         '(': 1,\n",
       "         'towing': 1,\n",
       "         'running': 1,\n",
       "         'person': 1,\n",
       "         'boat': 1,\n",
       "         'free-falling': 1,\n",
       "         'anchors': 1,\n",
       "         'paragliders': 1,\n",
       "         'fugitive': 1,\n",
       "         'parakites': 1,\n",
       "         'vehicle': 1,\n",
       "         ')': 1,\n",
       "         '.': 2,\n",
       "         'principles': 1,\n",
       "         'fluid': 1,\n",
       "         'flow': 1,\n",
       "         'apply': 1,\n",
       "         'liquids': 1,\n",
       "         'used': 2,\n",
       "         'water.': 1,\n",
       "         'hybrid': 1,\n",
       "         'comprising': 1,\n",
       "         'lighter-than-air': 1,\n",
       "         'balloon': 1,\n",
       "         'well': 1,\n",
       "         'lifting': 1,\n",
       "         'kytoon.': 1,\n",
       "         'long': 1,\n",
       "         'varied': 1,\n",
       "         'history': 1,\n",
       "         'many': 1,\n",
       "         'different': 1,\n",
       "         'types': 1,\n",
       "         'flown': 3,\n",
       "         'individually': 1,\n",
       "         'festivals': 1,\n",
       "         'worldwide.': 1,\n",
       "         'recreation': 1,\n",
       "         'art': 1,\n",
       "         'practical': 1,\n",
       "         'uses.': 1,\n",
       "         'sport': 1,\n",
       "         'aerial': 1,\n",
       "         'ballet': 1,\n",
       "         'sometimes': 1,\n",
       "         'part': 1,\n",
       "         'competition.': 1,\n",
       "         'power': 2,\n",
       "         'multi-line': 1,\n",
       "         'steerable': 1,\n",
       "         'generate': 1,\n",
       "         'large': 1,\n",
       "         'forces': 1,\n",
       "         'activities': 1,\n",
       "         'surfing': 1,\n",
       "         'landboarding': 1,\n",
       "         'fishing': 1,\n",
       "         'buggying': 1,\n",
       "         'new': 1,\n",
       "         'trend': 1,\n",
       "         'snow': 1,\n",
       "         'kiting.': 1,\n",
       "         'man-lifting': 1,\n",
       "         'made': 1})"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse\n",
    "kite_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing the first 10 values of the list\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.75, 1.75, 1.0, 0.625, 0.5, 0.5, 0.375, 0.375, 0.375, 0.25]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_vector = []\n",
    "doc_length = len(tokens)\n",
    "for key, value in kite_counts.most_common():\n",
    "    document_vector.append(value / doc_length)\n",
    "print('Printing the first 10 values of the list')\n",
    "document_vector[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lexicon with a new corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tokens of the first term in the list: 17\n",
      "number of total tokens: 33\n",
      "**************************************************\n",
      "size of the lexicon: 18\n",
      "lexicon: [',', '.', 'and', 'as', 'faster', 'get', 'got', 'hairy', 'harry', 'home', 'is', 'jill', 'not', 'score', 'than', 'the', 'to', 'would']\n"
     ]
    }
   ],
   "source": [
    "# corpus\n",
    "docs = [\"The faster Harry got to the score, the faster and faster Harry would get home.\"]\n",
    "docs.append(\"Harry is hairy and faster than Jill.\")\n",
    "docs.append(\"Jill is not as hairy as Harry.\")\n",
    "\n",
    "doc_tokens = []\n",
    "for doc in docs:\n",
    "    doc_tokens += [sorted(tokenizer.tokenize(doc.lower()))]\n",
    "print('number of tokens of the first term in the list:', len(doc_tokens[0]))\n",
    "\n",
    "all_doc_tokens = sum(doc_tokens, [])\n",
    "print('number of total tokens:', len(all_doc_tokens))\n",
    "\n",
    "lexicon = sorted(set(all_doc_tokens))\n",
    "print('**************************************************')\n",
    "print('size of the lexicon:', len(lexicon))\n",
    "print('lexicon:', lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vectorize the sentences in a list of OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printint the First OrderedDict that corresponds to the first sentence:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([(',', 0),\n",
       "             ('.', 0.05555555555555555),\n",
       "             ('and', 0),\n",
       "             ('as', 0.1111111111111111),\n",
       "             ('faster', 0),\n",
       "             ('get', 0),\n",
       "             ('got', 0),\n",
       "             ('hairy', 0.05555555555555555),\n",
       "             ('harry', 0.05555555555555555),\n",
       "             ('home', 0),\n",
       "             ('is', 0.05555555555555555),\n",
       "             ('jill', 0.05555555555555555),\n",
       "             ('not', 0.05555555555555555),\n",
       "             ('score', 0),\n",
       "             ('than', 0),\n",
       "             ('the', 0),\n",
       "             ('to', 0),\n",
       "             ('would', 0)])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "zero_vector = OrderedDict((token, 0) for token in lexicon)\n",
    "\n",
    "import copy\n",
    "doc_vectors = []\n",
    "for doc in docs:\n",
    "    vec = copy.copy(zero_vector)\n",
    "    tokens = tokenizer.tokenize(doc.lower())\n",
    "    token_counts = Counter(tokens)\n",
    "    for key, value in token_counts.items():\n",
    "        vec[key] = value / len(lexicon)\n",
    "    doc_vectors.append(vec)\n",
    "\n",
    "print('Printint the First OrderedDict that corresponds to the first sentence:')\n",
    "doc_vectors[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CosineSimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def cosine_sim(vec1, vec2):\n",
    "    \"\"\" Let's convert our dictionaries to lists for easier matching.\"\"\"\n",
    "    vec1 = [val for val in vec1.values()]\n",
    "    vec2 = [val for val in vec2.values()]\n",
    "\n",
    "    dot_prod = 0\n",
    "    for i, v in enumerate(vec1):\n",
    "        dot_prod += v * vec2[i]\n",
    "\n",
    "    mag_1 = math.sqrt(sum([x**2 for x in vec1]))\n",
    "    mag_2 = math.sqrt(sum([x**2 for x in vec2]))\n",
    "    return dot_prod / (mag_1 * mag_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1st approach TF-IDF, the harder version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "kite_history = \"\"\"Kites were invented in China, where materials ideal for kite building were readily \n",
    "available: silk fabric for sail material; fine, high-tensile-strength silk for flying line; and resilient\n",
    "bamboo for a strong, lightweight framework.\n",
    "The kite has been claimed as the invention of the 5th-century BC Chinese philosophers Mozi (also Mo Di) \n",
    "and Lu Ban (also Gongshu Ban). By 549 AD paper kites were certainly being flown, as it was recorded that \n",
    "in that year a paper kite was used as a message for a rescue mission. Ancient and medieval Chinese sources\n",
    "describe kites being used for measuring distances, testing the wind, lifting men, signaling, and \n",
    "communication for military operations. The earliest known Chinese kites were flat (not bowed) and \n",
    "often rectangular. Later, tailless kites incorporated a stabilizing bowline. Kites were decorated with \n",
    "mythological motifs and legendary figures; some were fitted with strings and whistles to make musical \n",
    "sounds while flying. From China, kites were introduced to Cambodia, Thailand, India, Japan, Korea and \n",
    "the western world.\n",
    "After its introduction into India, the kite further evolved into the fighter kite, known as the patang \n",
    "in India, where thousands are flown every year on festivals such as Makar Sankranti.\n",
    "Kites were known throughout Polynesia, as far as New Zealand, with the assumption being that the \n",
    "knowledge diffused from China along with the people. Anthropomorphic kites made from cloth and wood \n",
    "were used in religious ceremonies to send prayers to the gods. Polynesian kite traditions are used by \n",
    "anthropologists get an idea of early “primitive” Asian traditions that are believed to have at one time \n",
    "existed in Asia.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "detect a list of tokens from texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens in kite text: 361\n",
      "tokens in kite history: 295\n",
      "\n",
      "This section is to show what zipf's law is\n",
      "Term Frequency of word \"kite\" in intro document is: 0.0388\n",
      "Term Frequency of word \"kite\" in history document is: 0.0203\n",
      "Term Frequency of word \"and\" in intro document is: 0.0277\n",
      "Term Frequency of word \"and\" in history document is: 0.0305\n",
      "\n"
     ]
    }
   ],
   "source": [
    "intro_tokens = tokenizer.tokenize(kite_text.lower())\n",
    "history_tokens = tokenizer.tokenize(kite_history.lower())\n",
    "intro_total = len(intro_tokens)\n",
    "history_total = len(history_tokens)\n",
    "print('tokens in kite text:', intro_total)\n",
    "print('tokens in kite history:', history_total)\n",
    "print('')\n",
    "\n",
    "print(\"This section is to show what zipf's law is\")\n",
    "# compute the TF of one word in two documents\n",
    "intro_tf = {}\n",
    "history_tf = {}\n",
    "intro_counts = Counter(intro_tokens)\n",
    "intro_tf['kite'] = intro_counts['kite'] / intro_total\n",
    "history_counts = Counter(history_tokens)\n",
    "history_tf['kite'] = history_counts['kite'] / history_total\n",
    "\n",
    "# the tf of word \"kite\" in document A is twice times the tf of word kite in document B \n",
    "# (according to zipf's law)\n",
    "print('Term Frequency of word \"kite\" in intro document is: {:.4f}'.format(intro_tf['kite']))\n",
    "print('Term Frequency of word \"kite\" in history document is: {:.4f}'.format(history_tf['kite']))\n",
    "\n",
    "# But zipf's law is not fulfilled with word \"and\"\n",
    "intro_tf['and'] = intro_counts['and'] / intro_total\n",
    "history_tf['and'] = history_counts['and'] / history_total\n",
    "print('Term Frequency of word \"and\" in intro document is: {:.4f}'.format(intro_tf['and']))\n",
    "print('Term Frequency of word \"and\" in history document is: {:.4f}'.format(history_tf['and']))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculating the TF-IDF of only 3 words: \"and\", \"kite\" and \"china\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF of the words in document intro is:  {'and': 0.027700831024930747, 'kite': 0.038781163434903045, 'china': 0.0}\n",
      "TF-IDF of the words in documenty history is  {'and': 0.030508474576271188, 'kite': 0.020338983050847456, 'china': 0.010169491525423728}\n"
     ]
    }
   ],
   "source": [
    "num_docs_containing_and = 0\n",
    "for doc in [intro_tokens, history_tokens]:\n",
    "    if 'and' in doc:\n",
    "        num_docs_containing_and += 1\n",
    "\n",
    "num_docs_containing_kite = 0\n",
    "for doc in [intro_tokens, history_tokens]:\n",
    "    if 'and' in doc:\n",
    "        num_docs_containing_kite += 1\n",
    "\n",
    "num_docs_containing_china = 0\n",
    "for doc in [intro_tokens, history_tokens]:\n",
    "    if 'and' in doc:\n",
    "        num_docs_containing_china += 1\n",
    "\n",
    "intro_tf['china'] = intro_counts['china'] / intro_total\n",
    "history_tf['china'] = history_counts['china'] / history_total\n",
    "num_docs = 2\n",
    "intro_idf = {}\n",
    "history_idf = {}\n",
    "intro_idf['and'] = num_docs / num_docs_containing_and\n",
    "history_idf['and'] = num_docs / num_docs_containing_and\n",
    "intro_idf['kite'] = num_docs / num_docs_containing_kite\n",
    "history_idf['kite'] = num_docs / num_docs_containing_kite\n",
    "intro_idf['china'] = num_docs / num_docs_containing_china\n",
    "history_idf['china'] = num_docs / num_docs_containing_china\n",
    "\n",
    "intro_tfidf = {}\n",
    "intro_tfidf['and'] = intro_tf['and'] * intro_idf['and']\n",
    "intro_tfidf['kite'] = intro_tf['kite'] * intro_idf['kite']\n",
    "intro_tfidf['china'] = intro_tf['china'] * intro_idf['china']\n",
    "history_tfidf = {}\n",
    "history_tfidf['and'] = history_tf['and'] * history_idf['and']\n",
    "history_tfidf['kite'] = history_tf['kite'] * history_idf['kite']\n",
    "history_tfidf['china'] = history_tf['china'] * history_idf['china']\n",
    "print('TF-IDF of the words in document intro is: ', intro_tfidf)\n",
    "print('TF-IDF of the words in documenty history is ',history_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2nd approach TF-IDF with similarities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "converting a list of text into a list of tuples containing the token and its count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus: ['The faster Harry got to the score, the faster and faster Harry would get home.', 'Harry is hairy and faster than Jill.', 'Jill is not as hairy as Harry.']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[OrderedDict([(',', 0.16666666666666666),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0.08333333333333333),\n",
       "              ('as', 0),\n",
       "              ('faster', 0.25),\n",
       "              ('get', 0.16666666666666666),\n",
       "              ('got', 0.16666666666666666),\n",
       "              ('hairy', 0),\n",
       "              ('harry', 0.0),\n",
       "              ('home', 0.16666666666666666),\n",
       "              ('is', 0),\n",
       "              ('jill', 0),\n",
       "              ('not', 0),\n",
       "              ('score', 0.16666666666666666),\n",
       "              ('than', 0),\n",
       "              ('the', 0.5),\n",
       "              ('to', 0.16666666666666666),\n",
       "              ('would', 0.16666666666666666)]),\n",
       " OrderedDict([(',', 0),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0.08333333333333333),\n",
       "              ('as', 0),\n",
       "              ('faster', 0.08333333333333333),\n",
       "              ('get', 0),\n",
       "              ('got', 0),\n",
       "              ('hairy', 0.08333333333333333),\n",
       "              ('harry', 0.0),\n",
       "              ('home', 0),\n",
       "              ('is', 0.08333333333333333),\n",
       "              ('jill', 0.0),\n",
       "              ('not', 0),\n",
       "              ('score', 0),\n",
       "              ('than', 0.16666666666666666),\n",
       "              ('the', 0),\n",
       "              ('to', 0),\n",
       "              ('would', 0)]),\n",
       " OrderedDict([(',', 0),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0),\n",
       "              ('as', 0.1111111111111111),\n",
       "              ('faster', 0),\n",
       "              ('get', 0),\n",
       "              ('got', 0),\n",
       "              ('hairy', 0.08333333333333333),\n",
       "              ('harry', 0.0),\n",
       "              ('home', 0),\n",
       "              ('is', 0.08333333333333333),\n",
       "              ('jill', 0.0),\n",
       "              ('not', 0.16666666666666666),\n",
       "              ('score', 0),\n",
       "              ('than', 0),\n",
       "              ('the', 0),\n",
       "              ('to', 0),\n",
       "              ('would', 0)])]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "print('corpus:', docs)\n",
    "print('')\n",
    "document_tfidf_vectors = []\n",
    "for doc in docs:\n",
    "    vec = copy.copy(zero_vector)\n",
    "    tokens = tokenizer.tokenize(doc.lower())\n",
    "    token_counts = Counter(tokens)\n",
    "\n",
    "    for key, value in token_counts.items():\n",
    "        docs_containing_key = 0\n",
    "        for _doc in docs:\n",
    "            if key in _doc:\n",
    "                docs_containing_key += 1\n",
    "        tf = value / len(lexicon)\n",
    "        if docs_containing_key:\n",
    "            idf = len(docs) / docs_containing_key\n",
    "        else:\n",
    "            idf = 0\n",
    "        vec[key] = tf * idf\n",
    "    document_tfidf_vectors.append(vec)\n",
    "document_tfidf_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF of query(list version): OrderedDict([(',', 0), ('.', 0), ('and', 0), ('as', 0), ('faster', 0), ('get', 0.2727272727272727), ('got', 0), ('hairy', 0), ('harry', 0), ('home', 0), ('is', 0), ('jill', 0), ('not', 0), ('score', 0), ('than', 0), ('the', 0.2727272727272727), ('to', 0.5454545454545454), ('would', 0)])\n"
     ]
    }
   ],
   "source": [
    "query = \"How long does it take to get to the store?\"\n",
    "query_vec = copy.copy(zero_vector)\n",
    "\n",
    "# normalize the query\n",
    "tokens = tokenizer.tokenize(query.lower())\n",
    "# a dic with tokens with counts\n",
    "token_counts = Counter(tokens)\n",
    "\n",
    "documents = docs\n",
    "for key, value in token_counts.items():\n",
    "    docs_containing_key = 0\n",
    "    for _doc in documents:\n",
    "        if key in _doc.lower():\n",
    "            docs_containing_key += 1\n",
    "    # Avoiding divide-by-zero-error\n",
    "    if docs_containing_key == 0:\n",
    "        continue\n",
    "    tf = value / len(tokens)\n",
    "    idf = len(documents) / docs_containing_key\n",
    "    query_vec[key] = tf * idf\n",
    "\n",
    "print('TF-IDF of query(list version):',query_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "computing similarities between documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarities between query and 1st, 2nd and 3rd document:\n",
      "0.5677922680888605\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print('similarities between query and 1st, 2nd and 3rd document:')\n",
    "print(cosine_sim(query_vec, document_tfidf_vectors[0]))\n",
    "print(cosine_sim(query_vec, document_tfidf_vectors[1]))\n",
    "print(cosine_sim(query_vec, document_tfidf_vectors[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3rd Approach - The best version with sckit learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus ndarray output:\n",
      "message: need to include the similarities (like 2nd approach)betweenTF-IDF vectors: query and corpus!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.16, 0.  , 0.48, 0.21, 0.21, 0.  , 0.25, 0.21, 0.  , 0.  , 0.  ,\n",
       "        0.21, 0.  , 0.64, 0.21, 0.21],\n",
       "       [0.37, 0.  , 0.37, 0.  , 0.  , 0.37, 0.29, 0.  , 0.37, 0.37, 0.  ,\n",
       "        0.  , 0.49, 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.75, 0.  , 0.  , 0.  , 0.29, 0.22, 0.  , 0.29, 0.29, 0.38,\n",
       "        0.  , 0.  , 0.  , 0.  , 0.  ]])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = docs\n",
    "vectorizer = TfidfVectorizer(min_df=1)\n",
    "model = vectorizer.fit_transform(corpus)\n",
    "print('corpus ndarray output:')\n",
    "print('message: need to include the similarities(like 2nd approach)between'\n",
    "      +'TF-IDF vectors: query and corpus!')\n",
    "model.todense().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Meanings in Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spam classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of spam mails 4837\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spam</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sms0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms2!</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms5!</th>\n",
       "      <td>1</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       spam                                               text\n",
       "sms0      0  Go until jurong point, crazy.. Available only ...\n",
       "sms1      0                      Ok lar... Joking wif u oni...\n",
       "sms2!     1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "sms3      0  U dun say so early hor... U c already then say...\n",
       "sms4      0  Nah I don't think he goes to usf, he lives aro...\n",
       "sms5!     1  FreeMsg Hey there darling it's been 3 week's n..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nlpia.data.loaders import get_data\n",
    "pd.options.display.width = 120\n",
    "sms = get_data('sms-spam')\n",
    "\n",
    "index = ['sms{}{}'.format(i, '!'*j) for (i,j) in zip(range(len(sms)), sms.spam)]\n",
    "sms = pd.DataFrame(sms.values, columns=sms.columns, index=index)\n",
    "sms['spam'] = sms.spam.astype(int)\n",
    "print('number of spam messages', len(sms))\n",
    "sms.spam.sum()\n",
    "\n",
    "sms.head(6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
